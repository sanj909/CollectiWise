{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "first_attempt_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyODt+WF7+K6y+atqT9q/x+x"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpqJm_LHBvQ-",
        "outputId": "de2ed91f-f695-485b-c108-2bb4ea5e2cbf"
      },
      "source": [
        "# Previous Check to have a GPU available\r\n",
        "# Make sure that you have GPU selected in the Runtime\r\n",
        "# If you do, will print Found GPU at: /device:GPU:0\r\n",
        "# Else go to Runtime -> Change Runtime Type\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "if device_name != '/device:GPU:0':\r\n",
        "  raise SystemError('GPU device not found')\r\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogO9acd4CCEp",
        "outputId": "87eeade3-2872-42cf-8d85-8c5f2b0cc382"
      },
      "source": [
        "# GOOGLE DRIVE - COLAB\r\n",
        "# Load the Drive helper and mount\r\n",
        "from google.colab import drive\r\n",
        "\r\n",
        "# This will prompt for authorization.\r\n",
        "# drive.mount('/content/drive')\r\n",
        "drive.mount(\"/content/drive\", force_remount=True)\r\n",
        "\r\n",
        "# After executing the cell above, Drive\r\n",
        "# files will be present in \"/content/drive/My Drive\".\r\n",
        "# !ls \"/content/drive/My Drive\"\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS5EXJ-_CpuO"
      },
      "source": [
        "# googlepath exists for googledrive finding the files. Can be set to empty \r\n",
        "# string if running on computer. Have companylist.csv in same dir. Will create\r\n",
        "# a stock_data folder and download stocks data into that. \r\n",
        "googlepath = \"drive/My Drive/Colab Notebooks/GAN cryptocurrency/\"\r\n",
        "\r\n",
        "# Setting the Training Amount\r\n",
        "TRAINING_AMOUNT = 50000 # low to test for now\r\n",
        "SAVE_STEPS_AMOUNT = 10000 # testing for now\r\n",
        "PCT_CHANGE_AMOUNT = 0.001 # just want to see up down trends ORIGINAL 5\r\n",
        "HISTORICAL_DAYS_AMOUNT = 20\r\n",
        "DAYS_AHEAD = 1 # ORIGINAL 5"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpKyem8lCtjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bcd0a6-43db-44bc-ab5b-f9a596ce1449"
      },
      "source": [
        "'''\r\n",
        "Downloads stock data from alphavantage\r\n",
        "'''\r\n",
        "import pandas as pd \r\n",
        "import os\r\n",
        "import time\r\n",
        "import urllib\r\n",
        "import json\r\n",
        "import csv\r\n",
        "import requests\r\n",
        "import io\r\n",
        "from pathlib import Path\r\n",
        "import random\r\n",
        "\r\n",
        "ALPHA_VANTAGE_KEY = \"3RZHI30EBWV3VN9R\"\r\n",
        "\r\n",
        "'''\r\n",
        "Note should have companylist.csv in the directory with this file.\r\n",
        "'''\r\n",
        "\r\n",
        "'''\r\n",
        "Saves data to a file\r\n",
        "'''\r\n",
        "def save(googlepath, stock_csv, output_dir, filename):\r\n",
        "    try:\r\n",
        "        #the output dir may not exist\r\n",
        "        if not os.path.exists(output_dir):\r\n",
        "            os.makedirs(output_dir)\r\n",
        "    except Exception as ex:\r\n",
        "        print('Could not create output dir')\r\n",
        "        print(ex)\r\n",
        "        return\r\n",
        "    filepath = os.path.join(googlepath, output_dir, filename)\r\n",
        "    try:\r\n",
        "#         print(stock_csv)\r\n",
        "        df = stock_csv\r\n",
        "        df = df.sort_values(by='timestamp')  \r\n",
        "#         print(df)\r\n",
        "        df.to_csv(filepath, index=False)\r\n",
        "    except Exception as ex:\r\n",
        "        print('Could not open file {} to write data'.format(filepath))\r\n",
        "        print(ex)\r\n",
        "\r\n",
        "\r\n",
        "def try_download(symbol):\r\n",
        "    try:\r\n",
        "        # Keep call frequency below threshold \r\n",
        "        time.sleep(12)    \r\n",
        "        url = 'https://www.alphavantage.co/query?function=DIGITAL_CURRENCY_DAILY&symbol=BTC&market=CNY&apikey=demo&datatype=csv'.format(symbol, ALPHA_VANTAGE_KEY)\r\n",
        "        c = pd.read_csv(url)\r\n",
        "        # getting rid of some columns won't look at for now\r\n",
        "        c = c.drop(['open (CNY)', 'high (CNY)', 'low (CNY)','close (CNY)','market cap (USD)'\t], axis=1)\r\n",
        "        c.columns=['timestamp','open','high','low','close','volume']\r\n",
        "        return c, True\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)\r\n",
        "        return None, None\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#Given a stock symbol (aka 'tsla') will download and save the data to the\r\n",
        "#output dir as a csv \r\n",
        "\r\n",
        "def download_symbol(symbol, output_dir, retry_count=4):\r\n",
        "\r\n",
        "    stock_csv, didPass = try_download(symbol)\r\n",
        "    if didPass:\r\n",
        "        save(googlepath, stock_csv, output_dir, '{}.csv'.format(symbol))\r\n",
        "    else:\r\n",
        "        print('Failed to download {}'.format(symbol))\r\n",
        "\r\n",
        "\r\n",
        "url = 'https://raw.githubusercontent.com/madt2709/easycode/master/List%20of%20cryptocurrencies.csv?token=ANKDBDTFBRBOYJJHEATNHTDAGPKMG'\r\n",
        "df = pd.read_csv(url)\r\n",
        "for symbol in df.Symbol:\r\n",
        "    my_file = Path(f\"{googlepath}stock_data/{symbol}.csv\")  # check if already downloaded\r\n",
        "    print(my_file.exists())\r\n",
        "    if not my_file.exists():\r\n",
        "        print('Downloading {}'.format(symbol))\r\n",
        "        download_symbol(symbol, 'stock_data')\r\n",
        "    else:\r\n",
        "        print(f\"Already downloaded {symbol}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "Downloading BTC\n",
            "False\n",
            "Downloading LTC\n",
            "False\n",
            "Downloading NMC\n",
            "False\n",
            "Downloading PPC\n",
            "False\n",
            "Downloading ETH\n",
            "False\n",
            "Downloading BNB\n",
            "False\n",
            "Downloading DOT\n",
            "False\n",
            "Downloading ADA\n",
            "False\n",
            "Downloading USDT\n",
            "False\n",
            "Downloading XRP\n",
            "False\n",
            "Downloading LINK\n",
            "False\n",
            "Downloading BCH\n",
            "False\n",
            "Downloading XLM\n",
            "False\n",
            "Downloading UNI\n",
            "False\n",
            "Downloading USDC\n",
            "False\n",
            "Downloading DOGE\n",
            "False\n",
            "Downloading WBTC\n",
            "False\n",
            "Downloading AAVE\n",
            "False\n",
            "Downloading ATOM\n",
            "False\n",
            "Downloading EOS\n",
            "False\n",
            "Downloading XEM\n",
            "False\n",
            "Downloading XMR\n",
            "False\n",
            "Downloading BSV\n",
            "False\n",
            "Downloading TRX\n",
            "False\n",
            "Downloading HT \n",
            "False\n",
            "Downloading MIOTA\n",
            "False\n",
            "Downloading XTZ\n",
            "False\n",
            "Downloading VET\n",
            "False\n",
            "Downloading THETA\n",
            "False\n",
            "Downloading CRO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSbnbBRkfz3Z"
      },
      "source": [
        "#Define the Confusion Matrix\r\n",
        "import itertools\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "\r\n",
        "def plot_confusion_matrix(cm, classes,\r\n",
        "                          normalize=False,\r\n",
        "                          title='Confusion matrix',\r\n",
        "                          cmap=plt.cm.Blues):\r\n",
        "    \"\"\"\r\n",
        "    This function prints and plots the confusion matrix.\r\n",
        "    Normalization can be applied by setting `normalize=True`.\r\n",
        "    \"\"\"\r\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n",
        "    plt.title(title)\r\n",
        "    plt.colorbar()\r\n",
        "    tick_marks = np.arange(len(classes))\r\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\r\n",
        "    plt.yticks(tick_marks, classes)\r\n",
        "    plt.ylim([1.5, -.5])\r\n",
        "\r\n",
        "\r\n",
        "    if normalize:\r\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n",
        "        print(\"Normalized confusion matrix\")\r\n",
        "    else:\r\n",
        "        print('Confusion matrix, without normalization')\r\n",
        "\r\n",
        "    print(cm)\r\n",
        "\r\n",
        "    #thresh = cm.max() / 2. # Previous\r\n",
        "    fmt = '.4f' if normalize else 'd'\r\n",
        "    thresh = cm.max() - 0.05\r\n",
        "\r\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n",
        "        #print (cm[i, j])\r\n",
        "        plt.text(j, i, format(cm[i, j],fmt),\r\n",
        "                 horizontalalignment=\"center\",\r\n",
        "                 fontsize=24, \r\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
        "\r\n",
        "    #for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n",
        "    #    plt.text(j, i, cm[i, j],\r\n",
        "    #             horizontalalignment=\"center\",\r\n",
        "    #             color=\"black\")\r\n",
        "\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.ylabel('True label')\r\n",
        "    plt.xlabel('Predicted label')\r\n",
        "    plt.show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00JWloBTgOJj"
      },
      "source": [
        "#Initializing the GAN\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import os\r\n",
        "\r\n",
        "SEED = 42\r\n",
        "tf.set_random_seed(SEED)\r\n",
        "\r\n",
        "class GAN():\r\n",
        "\r\n",
        "    def sample_Z(self, batch_size, n):\r\n",
        "        return np.random.uniform(-1., 1., size=(batch_size, n))\r\n",
        "\r\n",
        "    def __init__(self, num_features, num_historical_days, generator_input_size=200, is_train=True):\r\n",
        "        def get_batch_norm_with_global_normalization_vars(size):\r\n",
        "            v = tf.Variable(tf.ones([size]), dtype=tf.float32)\r\n",
        "            m = tf.Variable(tf.ones([size]), dtype=tf.float32)\r\n",
        "            beta = tf.Variable(tf.ones([size]), dtype=tf.float32)\r\n",
        "            gamma = tf.Variable(tf.ones([size]), dtype=tf.float32)\r\n",
        "            return v, m, beta, gamma\r\n",
        "\r\n",
        "        self.X = tf.placeholder(tf.float32, shape=[None, num_historical_days, num_features])\r\n",
        "        X = tf.reshape(self.X, [-1, num_historical_days, 1, num_features])\r\n",
        "        self.Z = tf.placeholder(tf.float32, shape=[None, generator_input_size])\r\n",
        "\r\n",
        "        generator_output_size = num_features*num_historical_days\r\n",
        "        with tf.variable_scope(\"generator\"):\r\n",
        "            W1 = tf.Variable(tf.truncated_normal([generator_input_size, generator_output_size*10]))\r\n",
        "            b1 = tf.Variable(tf.truncated_normal([generator_output_size*10]))\r\n",
        "\r\n",
        "            h1 = tf.nn.sigmoid(tf.matmul(self.Z, W1) + b1)\r\n",
        "\r\n",
        "            # v1, m1, beta1, gamma1 = get_batch_norm_with_global_normalization_vars(generator_output_size*10)\r\n",
        "            # h1 = tf.nn.batch_norm_with_global_normalization(h1, v1, m1,\r\n",
        "            #         beta1, gamma1, variance_epsilon=0.000001, scale_after_normalization=False)\r\n",
        "\r\n",
        "            W2 = tf.Variable(tf.truncated_normal([generator_output_size*10, generator_output_size*5]))\r\n",
        "            b2 = tf.Variable(tf.truncated_normal([generator_output_size*5]))\r\n",
        "\r\n",
        "            h2 = tf.nn.sigmoid(tf.matmul(h1, W2) + b2)\r\n",
        "\r\n",
        "            # v2, m2, beta2, gamma2 = get_batch_norm_with_global_normalization_vars(generator_output_size*5)\r\n",
        "            # h2 = tf.nn.batch_norm_with_global_normalization(h2, v2, m2,\r\n",
        "            #         beta2, gamma2, variance_epsilon=0.000001, scale_after_normalization=False)\r\n",
        "\r\n",
        "\r\n",
        "            W3 = tf.Variable(tf.truncated_normal([generator_output_size*5, generator_output_size]))\r\n",
        "            b3 = tf.Variable(tf.truncated_normal([generator_output_size]))\r\n",
        "\r\n",
        "            g_log_prob = tf.matmul(h2, W3) + b3\r\n",
        "            g_log_prob = tf.reshape(g_log_prob, [-1, num_historical_days, 1, num_features])\r\n",
        "            self.gen_data = tf.reshape(g_log_prob, [-1, num_historical_days, num_features])\r\n",
        "            #g_log_prob = g_log_prob / tf.reshape(tf.reduce_max(g_log_prob, axis=1), [-1, 1, num_features, 1])\r\n",
        "            #g_prob = tf.nn.sigmoid(g_log_prob)\r\n",
        "\r\n",
        "            theta_G = [W1, b1, W2, b2, W3, b3]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        with tf.variable_scope(\"discriminator\"):\r\n",
        "            #[filter_height, filter_width, in_channels, out_channels]\r\n",
        "            k1 = tf.Variable(tf.truncated_normal([3, 1, num_features, 32],\r\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\r\n",
        "            b1 = tf.Variable(tf.zeros([32], dtype=tf.float32))\r\n",
        "\r\n",
        "            v1, m1, beta1, gamma1 = get_batch_norm_with_global_normalization_vars(32)\r\n",
        "\r\n",
        "            k2 = tf.Variable(tf.truncated_normal([3, 1, 32, 64],\r\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\r\n",
        "            b2 = tf.Variable(tf.zeros([64], dtype=tf.float32))\r\n",
        "\r\n",
        "            v2, m2, beta2, gamma2 = get_batch_norm_with_global_normalization_vars(64)\r\n",
        "\r\n",
        "            k3 = tf.Variable(tf.truncated_normal([3, 1, 64, 128],\r\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\r\n",
        "            b3 = tf.Variable(tf.zeros([128], dtype=tf.float32))\r\n",
        "\r\n",
        "            v3, m3, beta3, gamma3 = get_batch_norm_with_global_normalization_vars(128)\r\n",
        "\r\n",
        "            W1 = tf.Variable(tf.truncated_normal([18*1*128, 128]))\r\n",
        "            b4 = tf.Variable(tf.truncated_normal([128]))\r\n",
        "\r\n",
        "            v4, m4, beta4, gamma4 = get_batch_norm_with_global_normalization_vars(128)\r\n",
        "\r\n",
        "            W2 = tf.Variable(tf.truncated_normal([128, 1]))\r\n",
        "\r\n",
        "            theta_D = [k1, b1, k2, b2, k3, b3, W1, b4, W2]\r\n",
        "\r\n",
        "        def discriminator(X):\r\n",
        "            conv = tf.nn.conv2d(X,k1,strides=[1, 1, 1, 1],padding='SAME')\r\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b1))\r\n",
        "            pool = relu\r\n",
        "            # pool = tf.nn.avg_pool(relu, ksize=[1, 2, 1, 1], strides=[1, 2, 1, 1], padding='SAME')\r\n",
        "            if is_train:\r\n",
        "                pool = tf.nn.dropout(pool, keep_prob = 0.8)\r\n",
        "            # pool = tf.nn.batch_norm_with_global_normalization(pool, v1, m1,\r\n",
        "            #         beta1, gamma1, variance_epsilon=0.000001, scale_after_normalization=False)\r\n",
        "            print(pool)\r\n",
        "\r\n",
        "            conv = tf.nn.conv2d(pool, k2,strides=[1, 1, 1, 1],padding='SAME')\r\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b2))\r\n",
        "            pool = relu\r\n",
        "            #pool = tf.nn.avg_pool(relu, ksize=[1, 2, 1, 1], strides=[1, 2, 1, 1], padding='SAME')\r\n",
        "            if is_train:\r\n",
        "                pool = tf.nn.dropout(pool, keep_prob = 0.8)\r\n",
        "            # pool = tf.nn.batch_norm_with_global_normalization(pool, v2, m2,\r\n",
        "            #         beta2, gamma2, variance_epsilon=0.000001, scale_after_normalization=False)\r\n",
        "            print(pool)\r\n",
        "\r\n",
        "            conv = tf.nn.conv2d(pool, k3, strides=[1, 1, 1, 1], padding='VALID')\r\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b3))\r\n",
        "            if is_train:\r\n",
        "                relu = tf.nn.dropout(relu, keep_prob=0.8)\r\n",
        "            # relu = tf.nn.batch_norm_with_global_normalization(relu, v3, m3,\r\n",
        "            #         beta3, gamma3, variance_epsilon=0.000001, scale_after_normalization=False)\r\n",
        "            print(relu)\r\n",
        "\r\n",
        "\r\n",
        "            flattened_convolution_size = int(relu.shape[1]) * int(relu.shape[2]) * int(relu.shape[3])\r\n",
        "            print(flattened_convolution_size)\r\n",
        "            flattened_convolution = features = tf.reshape(relu, [-1, flattened_convolution_size])\r\n",
        "\r\n",
        "            if is_train:\r\n",
        "                flattened_convolution =  tf.nn.dropout(flattened_convolution, keep_prob=0.8)\r\n",
        "\r\n",
        "            h1 = tf.nn.relu(tf.matmul(flattened_convolution, W1) + b4)\r\n",
        "\r\n",
        "            # h1 = tf.nn.batch_norm_with_global_normalization(h1, v4, m4,\r\n",
        "            #         beta4, gamma4, variance_epsilon=0.000001, scale_after_normalization=False)\r\n",
        "\r\n",
        "            D_logit = tf.matmul(h1, W2)\r\n",
        "            D_prob = tf.nn.sigmoid(D_logit)\r\n",
        "            return D_prob, D_logit, features\r\n",
        "\r\n",
        "        D_real, D_logit_real, self.features = discriminator(X)\r\n",
        "        D_fake, D_logit_fake, _ = discriminator(g_log_prob)\r\n",
        "\r\n",
        "\r\n",
        "        D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\r\n",
        "        D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\r\n",
        "        self.D_l2_loss = (0.0001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_D]) / len(theta_D))\r\n",
        "        self.D_loss = D_loss_real + D_loss_fake + self.D_l2_loss\r\n",
        "        self.G_l2_loss = (0.00001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_G]) / len(theta_G))\r\n",
        "        self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) + self.G_l2_loss\r\n",
        "\r\n",
        "\r\n",
        "        self.D_solver = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.D_loss, var_list=theta_D)\r\n",
        "        self.G_solver = tf.train.AdamOptimizer(learning_rate=0.000055).minimize(self.G_loss, var_list=theta_G)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EaD5Etbygzb_",
        "outputId": "4da22307-97b6-483e-f25d-ae9a5bbc08ae"
      },
      "source": [
        "#Training the GAN\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "# from gan import GAN\r\n",
        "import random\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n",
        "\r\n",
        "random.seed(42)\r\n",
        "class TrainGan:\r\n",
        "\r\n",
        "    def __init__(self, num_historical_days, batch_size=128):\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.data = []\r\n",
        "#         files = [os.path.join('./stock_data', f) for f in os.listdir('./stock_data')]\r\n",
        "\r\n",
        "        # Google Drive Method\r\n",
        "        files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")]\r\n",
        "#         print(files)\r\n",
        "      \r\n",
        "        for file in files:\r\n",
        "            print(file)\r\n",
        "            #Read in file -- note that parse_dates will be need later\r\n",
        "            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\r\n",
        "            df = df[['open','high','low','close','volume']]\r\n",
        "            # #Create new index with missing days\r\n",
        "            # idx = pd.date_range(df.index[-1], df.index[0])\r\n",
        "            # #Reindex and fill the missing day with the value from the day before\r\n",
        "            # df = df.reindex(idx, method='bfill').sort_index(ascending=False)\r\n",
        "            #Normilize using a of size num_historical_days\r\n",
        "            df = ((df -\r\n",
        "            df.rolling(num_historical_days).mean().shift(-num_historical_days))\r\n",
        "            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\r\n",
        "            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\r\n",
        "            #Drop the last 10 day that we don't have data for\r\n",
        "            df = df.dropna()\r\n",
        "            #Hold out the last year of trading for testing\r\n",
        "            #Padding to keep labels from bleeding\r\n",
        "            df = df[400:]\r\n",
        "            #This may not create good samples if num_historical_days is a\r\n",
        "            #mutliple of 7\r\n",
        "            for i in range(num_historical_days, len(df), num_historical_days):\r\n",
        "                self.data.append(df.values[i-num_historical_days:i])\r\n",
        "\r\n",
        "        self.gan = GAN(num_features=5, num_historical_days=num_historical_days,\r\n",
        "                        generator_input_size=200)\r\n",
        "\r\n",
        "    def random_batch(self, batch_size=128):\r\n",
        "        batch = []\r\n",
        "        while True:\r\n",
        "            batch.append(random.choice(self.data))\r\n",
        "            if (len(batch) == batch_size):\r\n",
        "                yield batch\r\n",
        "                batch = []\r\n",
        "\r\n",
        "    def train(self, print_steps=100, display_data=100, save_steps=SAVE_STEPS_AMOUNT):\r\n",
        "        if not os.path.exists(f'{googlepath}models'):\r\n",
        "            os.makedirs(f'{googlepath}models')\r\n",
        "        sess = tf.Session()\r\n",
        "        \r\n",
        "        G_loss = 0\r\n",
        "        D_loss = 0\r\n",
        "        G_l2_loss = 0\r\n",
        "        D_l2_loss = 0\r\n",
        "        sess.run(tf.global_variables_initializer())\r\n",
        "        saver = tf.train.Saver()\r\n",
        "        currentStep = \"0\"\r\n",
        "        \r\n",
        "        g_loss_array = []\r\n",
        "        d_loss_array = []\r\n",
        "        \r\n",
        "        if os.path.exists(f'{googlepath}models/checkpoint'):\r\n",
        "                with open(f'{googlepath}models/checkpoint', 'rb') as f:\r\n",
        "                    model_name = next(f).split('\"'.encode())[1]\r\n",
        "                filename = \"{}models/{}\".format(googlepath, model_name.decode())\r\n",
        "                currentStep = filename.split(\"-\")[1]\r\n",
        "                new_saver = tf.train.import_meta_graph('{}.meta'.format(filename))\r\n",
        "                new_saver.restore(sess, \"{}\".format(filename))\r\n",
        "\r\n",
        "        for i, X in enumerate(self.random_batch(self.batch_size)):\r\n",
        "\r\n",
        "            \r\n",
        "            \r\n",
        "            \r\n",
        "            if i % 1 == 0:\r\n",
        "                _, D_loss_curr, D_l2_loss_curr = sess.run([self.gan.D_solver, self.gan.D_loss, self.gan.D_l2_loss], feed_dict=\r\n",
        "                        {self.gan.X:X, self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\r\n",
        "                D_loss += D_loss_curr\r\n",
        "                D_l2_loss += D_l2_loss_curr\r\n",
        "            if i % 1 == 0:\r\n",
        "                _, G_loss_curr, G_l2_loss_curr = sess.run([self.gan.G_solver, self.gan.G_loss, self.gan.G_l2_loss],\r\n",
        "                        feed_dict={self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\r\n",
        "                G_loss += G_loss_curr\r\n",
        "                G_l2_loss += G_l2_loss_curr\r\n",
        "                \r\n",
        "            g_loss_array.append(G_loss_curr - G_l2_loss)\r\n",
        "            d_loss_array.append(D_loss_curr - D_l2_loss)\r\n",
        "            \r\n",
        "            \r\n",
        "            if (i+1) % print_steps == 0:\r\n",
        "                print('Step={} D_loss={}, G_loss={}'.format(i + int(currentStep), D_loss/print_steps - D_l2_loss/print_steps, G_loss/print_steps - G_l2_loss/print_steps))\r\n",
        "                #print('D_l2_loss = {} G_l2_loss={}'.format(D_l2_loss/print_steps, G_l2_loss/print_steps))\r\n",
        "                G_loss = 0\r\n",
        "                D_loss = 0\r\n",
        "                G_l2_loss = 0\r\n",
        "                D_l2_loss = 0\r\n",
        "            if (i+1) % save_steps == 0:\r\n",
        "                saver.save(sess, f'{googlepath}/models/gan.ckpt', i + int(currentStep))\r\n",
        "            \r\n",
        "            # end training at training_amount epochs\r\n",
        "            if ((i + int(currentStep)) > TRAINING_AMOUNT):\r\n",
        "                \r\n",
        "                print(\"Reached {} epochs for GAN\".format(i + int(currentStep)))\r\n",
        "                sess.close()\r\n",
        "                \r\n",
        "                axisX = np.arange(0,len(g_loss_array),1)\r\n",
        "                plt.plot(axisX, g_loss_array, label='generator loss')\r\n",
        "                plt.plot(axisX, d_loss_array, label='discriminator loss')\r\n",
        "                plt.legend()\r\n",
        "                plt.title('generator and discriminator loss')\r\n",
        "                plt.show()\r\n",
        "                \r\n",
        "                break\r\n",
        "\r\n",
        "            # if (i+1) % display_data == 0:\r\n",
        "            #     print('Generated Data')\r\n",
        "            #     print(sess.run(self.gan.gen_data, feed_dict={self.gan.Z:self.gan.sample_Z(1, 200)}))\r\n",
        "            #     print('Real Data')\r\n",
        "            #     print(X[0])\r\n",
        "\r\n",
        "\r\n",
        "# if __name__ == '__main__':\r\n",
        "tf.compat.v1.reset_default_graph()\r\n",
        "gan = TrainGan(HISTORICAL_DAYS_AMOUNT, 128)\r\n",
        "gan.train()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BTC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/LTC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/NMC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/PPC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/ETH.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BNB.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/DOT.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/ADA.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/USDT.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XRP.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/LINK.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BCH.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XLM.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/UNI.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/USDC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/DOGE.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/WBTC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/AAVE.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/ATOM.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/EOS.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XEM.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XMR.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BSV.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/TRX.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/HT .csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/MIOTA.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XTZ.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/VET.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/THETA.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/CRO.csv\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Tensor(\"dropout/Mul_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"dropout_1/Mul_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"dropout_2/Mul_1:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"dropout_4/Mul_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"dropout_5/Mul_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"dropout_6/Mul_1:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Step=99 D_loss=20.093630982637407, G_loss=813.8716745105386\n",
            "Step=199 D_loss=3.1818357264995574, G_loss=804.1916608074307\n",
            "Step=299 D_loss=1.642166761159897, G_loss=768.3560044127703\n",
            "Step=399 D_loss=0.8211958205699919, G_loss=699.3358191621303\n",
            "Step=499 D_loss=0.5578426969051362, G_loss=664.525460897982\n",
            "Step=599 D_loss=0.3285370528697966, G_loss=649.2492058703302\n",
            "Step=699 D_loss=0.2829214930534363, G_loss=643.1192966651917\n",
            "Step=799 D_loss=0.1722986936569213, G_loss=675.0864274469018\n",
            "Step=899 D_loss=0.15868637561798105, G_loss=686.4112785053253\n",
            "Step=999 D_loss=0.11396770954132074, G_loss=709.43340100348\n",
            "Step=1099 D_loss=0.07809558272361738, G_loss=699.8751273179054\n",
            "Step=1199 D_loss=0.06206748604774481, G_loss=705.6270598086714\n",
            "Step=1299 D_loss=0.08165425300598139, G_loss=731.9947757905721\n",
            "Step=1399 D_loss=0.040841734409332364, G_loss=737.6328388914467\n",
            "Step=1499 D_loss=0.05262803077697753, G_loss=733.7644832822681\n",
            "Step=1599 D_loss=0.048505507707595985, G_loss=749.4367578825355\n",
            "Step=1699 D_loss=0.05727220416069034, G_loss=757.3096908348798\n",
            "Step=1799 D_loss=0.031143238544464102, G_loss=793.5778633782268\n",
            "Step=1899 D_loss=0.015488166809082049, G_loss=778.7357038420439\n",
            "Step=1999 D_loss=0.03285909533500675, G_loss=757.7667515695094\n",
            "Step=2099 D_loss=0.02457589507102953, G_loss=807.5510498082638\n",
            "Step=2199 D_loss=0.026323825120925903, G_loss=843.4875929403305\n",
            "Step=2299 D_loss=0.007262239456176767, G_loss=806.2424014651775\n",
            "Step=2399 D_loss=0.00867905855178841, G_loss=748.8156011009216\n",
            "Step=2499 D_loss=0.008668506145477206, G_loss=732.4046244385838\n",
            "Step=2599 D_loss=0.002858299016952559, G_loss=708.3064935779572\n",
            "Step=2699 D_loss=0.01580132722854599, G_loss=676.9140369075537\n",
            "Step=2799 D_loss=0.0035562765598298185, G_loss=652.923752977252\n",
            "Step=2899 D_loss=0.0011766660213470281, G_loss=594.0884262657166\n",
            "Step=2999 D_loss=0.006123872995376534, G_loss=539.5448796984554\n",
            "Step=3099 D_loss=0.007971864938735918, G_loss=516.0284801656007\n",
            "Step=3199 D_loss=0.008312624692916826, G_loss=469.4113034379482\n",
            "Step=3299 D_loss=0.003041169643402064, G_loss=416.6497554394603\n",
            "Step=3399 D_loss=0.013829871416091821, G_loss=391.8943314021826\n",
            "Step=3499 D_loss=0.006231631040573005, G_loss=355.66577071666717\n",
            "Step=3599 D_loss=0.010827488899230797, G_loss=311.8263518065214\n",
            "Step=3699 D_loss=0.023873809576034466, G_loss=264.35488591790204\n",
            "Step=3799 D_loss=0.0336672186851501, G_loss=221.27752836942673\n",
            "Step=3899 D_loss=0.16301904797554, G_loss=190.6938605943322\n",
            "Step=3999 D_loss=0.6983920025825501, G_loss=148.723664893806\n",
            "Step=4099 D_loss=0.5632202756404876, G_loss=94.85991460919381\n",
            "Step=4199 D_loss=0.594134795665741, G_loss=67.78133916795254\n",
            "Step=4299 D_loss=0.45103488206863407, G_loss=52.79782640576363\n",
            "Step=4399 D_loss=0.6284482944011689, G_loss=50.762744385600094\n",
            "Step=4499 D_loss=0.5600526344776153, G_loss=42.04206064194441\n",
            "Step=4599 D_loss=0.4542192721366882, G_loss=38.28556077182293\n",
            "Step=4699 D_loss=0.8740102815628052, G_loss=36.034473342001434\n",
            "Step=4799 D_loss=0.5732380986213683, G_loss=28.416121981143952\n",
            "Step=4899 D_loss=0.562794760465622, G_loss=26.908632420003414\n",
            "Step=4999 D_loss=0.6157931637763976, G_loss=25.63827472925186\n",
            "Step=5099 D_loss=0.6550547337532042, G_loss=20.989335979819298\n",
            "Step=5199 D_loss=0.7539145720005036, G_loss=19.24589570522308\n",
            "Step=5299 D_loss=0.4976423275470734, G_loss=18.76856069147587\n",
            "Step=5399 D_loss=0.6396437978744507, G_loss=18.42013876348734\n",
            "Step=5499 D_loss=0.4526968204975128, G_loss=16.28970099300146\n",
            "Step=5599 D_loss=0.5952038407325744, G_loss=17.170844723880293\n",
            "Step=5699 D_loss=0.6502285361289977, G_loss=14.320908453166485\n",
            "Step=5799 D_loss=0.5881673240661622, G_loss=13.923884674310685\n",
            "Step=5899 D_loss=0.7468641495704651, G_loss=11.950023534595967\n",
            "Step=5999 D_loss=0.7296250498294832, G_loss=12.394587295949458\n",
            "Step=6099 D_loss=0.5557614541053773, G_loss=11.80531909674406\n",
            "Step=6199 D_loss=0.8899998128414155, G_loss=9.580703203976155\n",
            "Step=6299 D_loss=0.4505182743072509, G_loss=11.550108302235603\n",
            "Step=6399 D_loss=0.3214295172691346, G_loss=12.280000701546669\n",
            "Step=6499 D_loss=0.7341028177738189, G_loss=10.183866710960865\n",
            "Step=6599 D_loss=0.5486352837085724, G_loss=10.31677248120308\n",
            "Step=6699 D_loss=0.6600000894069671, G_loss=9.625375738739967\n",
            "Step=6799 D_loss=1.082298012971878, G_loss=9.219733296632766\n",
            "Step=6899 D_loss=0.4913187825679779, G_loss=7.24597491890192\n",
            "Step=6999 D_loss=0.48109874725341806, G_loss=10.041143978536129\n",
            "Step=7099 D_loss=0.7468585145473481, G_loss=8.729429201483727\n",
            "Step=7199 D_loss=0.970463236570358, G_loss=8.593840577900409\n",
            "Step=7299 D_loss=0.6180630111694334, G_loss=9.08837994903326\n",
            "Step=7399 D_loss=0.6719064199924467, G_loss=8.365122462213039\n",
            "Step=7499 D_loss=0.7604256832599641, G_loss=9.922199943065642\n",
            "Step=7599 D_loss=0.959563695192337, G_loss=7.592880639731884\n",
            "Step=7699 D_loss=0.8878217470645906, G_loss=4.991214582920074\n",
            "Step=7799 D_loss=0.7968186366558077, G_loss=5.461222222447396\n",
            "Step=7899 D_loss=0.5364941406250001, G_loss=5.983191391825676\n",
            "Step=7999 D_loss=0.8846385180950167, G_loss=5.3038146111369135\n",
            "Step=8099 D_loss=0.7311927914619445, G_loss=4.519435917437077\n",
            "Step=8199 D_loss=0.6308374738693239, G_loss=5.081804462075233\n",
            "Step=8299 D_loss=0.987715666294098, G_loss=4.11705395847559\n",
            "Step=8399 D_loss=0.8747590458393097, G_loss=3.572315309345722\n",
            "Step=8499 D_loss=0.512333892583847, G_loss=3.81251742452383\n",
            "Step=8599 D_loss=0.7246971821784973, G_loss=5.489768032133579\n",
            "Step=8699 D_loss=0.5387341821193696, G_loss=3.944772129058838\n",
            "Step=8799 D_loss=1.013207721710205, G_loss=4.086073476970196\n",
            "Step=8899 D_loss=0.6955086135864259, G_loss=3.3563102462887766\n",
            "Step=8999 D_loss=0.8065393877029419, G_loss=3.2628529471158982\n",
            "Step=9099 D_loss=0.8267863214015962, G_loss=3.5698742282390596\n",
            "Step=9199 D_loss=0.7029547595977783, G_loss=3.412925063371658\n",
            "Step=9299 D_loss=0.9562325263023375, G_loss=4.538684179782868\n",
            "Step=9399 D_loss=0.5811683475971221, G_loss=5.241424298882484\n",
            "Step=9499 D_loss=1.0008115816116332, G_loss=4.812930942177773\n",
            "Step=9599 D_loss=0.8239101123809816, G_loss=3.867033086121082\n",
            "Step=9699 D_loss=0.5933847236633301, G_loss=3.322919546663761\n",
            "Step=9799 D_loss=0.6060124123096466, G_loss=3.3887954220175747\n",
            "Step=9899 D_loss=0.7938938188552858, G_loss=3.0618044528365136\n",
            "Step=9999 D_loss=0.6245700907707215, G_loss=3.615898185968399\n",
            "Step=10099 D_loss=0.6897476303577423, G_loss=3.738412738144398\n",
            "Step=10199 D_loss=0.677574474811554, G_loss=8.261728380918502\n",
            "Step=10299 D_loss=1.0548127436637877, G_loss=3.44126570045948\n",
            "Step=10399 D_loss=1.207401134967804, G_loss=3.6480788832902915\n",
            "Step=10499 D_loss=0.831567975282669, G_loss=3.944767377972603\n",
            "Step=10599 D_loss=0.7522328853607176, G_loss=3.094933972358704\n",
            "Step=10699 D_loss=0.5170143723487854, G_loss=3.7768344673514362\n",
            "Step=10799 D_loss=0.5544994854927063, G_loss=5.233473851382732\n",
            "Step=10899 D_loss=0.773401914834976, G_loss=3.1251077225804327\n",
            "Step=10999 D_loss=0.7956812405586242, G_loss=3.640207486152649\n",
            "Step=11099 D_loss=1.1147327327728271, G_loss=5.09463504999876\n",
            "Step=11199 D_loss=1.0427879345417022, G_loss=3.0637248951196674\n",
            "Step=11299 D_loss=0.7091440629959107, G_loss=3.9303880655765537\n",
            "Step=11399 D_loss=0.5212465882301331, G_loss=3.545342918336391\n",
            "Step=11499 D_loss=0.35121520757675184, G_loss=5.702239760756492\n",
            "Step=11599 D_loss=1.1648867309093474, G_loss=2.9303898841142653\n",
            "Step=11699 D_loss=0.8913749635219574, G_loss=2.3647625136375425\n",
            "Step=11799 D_loss=0.5661221528053284, G_loss=2.591048628091812\n",
            "Step=11899 D_loss=0.7862995076179504, G_loss=1.8649150237441061\n",
            "Step=11999 D_loss=0.5854669439792632, G_loss=2.4002436786890033\n",
            "Step=12099 D_loss=0.7879878759384156, G_loss=2.1293420198559763\n",
            "Step=12199 D_loss=0.5767437052726745, G_loss=2.798445967435837\n",
            "Step=12299 D_loss=0.7544058978557586, G_loss=2.3400335699319843\n",
            "Step=12399 D_loss=0.7042651402950286, G_loss=2.290679541826248\n",
            "Step=12499 D_loss=0.5072150492668153, G_loss=2.469290237426758\n",
            "Step=12599 D_loss=0.6836045777797699, G_loss=2.6349559092521666\n",
            "Step=12699 D_loss=0.5658744037151338, G_loss=4.289665893614292\n",
            "Step=12799 D_loss=0.6605381309986116, G_loss=4.214443872869015\n",
            "Step=12899 D_loss=0.6792892956733705, G_loss=2.592916083931923\n",
            "Step=12999 D_loss=0.6052879726886751, G_loss=2.5764102324843403\n",
            "Step=13099 D_loss=0.6536362993717193, G_loss=2.103529443442821\n",
            "Step=13199 D_loss=0.645006844997406, G_loss=2.811201042532921\n",
            "Step=13299 D_loss=0.4995338022708893, G_loss=4.164700237214565\n",
            "Step=13399 D_loss=0.817316926717758, G_loss=2.7691430377960202\n",
            "Step=13499 D_loss=0.5226672673225403, G_loss=4.014661673307419\n",
            "Step=13599 D_loss=0.34359294176101685, G_loss=2.733043602705002\n",
            "Step=13699 D_loss=0.718312257528305, G_loss=2.5322554501891132\n",
            "Step=13799 D_loss=0.965287219285965, G_loss=2.21242383480072\n",
            "Step=13899 D_loss=0.6498870640993117, G_loss=2.469168931543827\n",
            "Step=13999 D_loss=0.7236850440502167, G_loss=2.2193859565258025\n",
            "Step=14099 D_loss=0.5124447911977768, G_loss=2.4029669040441513\n",
            "Step=14199 D_loss=0.916712504029274, G_loss=2.5233739030361177\n",
            "Step=14299 D_loss=1.0420473301410673, G_loss=4.158202678263188\n",
            "Step=14399 D_loss=0.49044368982315056, G_loss=2.4020960336923602\n",
            "Step=14499 D_loss=0.7582469195127487, G_loss=2.1029403239488604\n",
            "Step=14599 D_loss=0.7057050722837448, G_loss=1.9628793907165525\n",
            "Step=14699 D_loss=0.6178517961502076, G_loss=2.094399400949478\n",
            "Step=14799 D_loss=0.6509240019321443, G_loss=2.131585058271885\n",
            "Step=14899 D_loss=0.5827802550792694, G_loss=1.9055753067135812\n",
            "Step=14999 D_loss=0.5623453050851821, G_loss=1.8661503037810325\n",
            "Step=15099 D_loss=0.534733902812004, G_loss=2.623358886837959\n",
            "Step=15199 D_loss=0.575985015630722, G_loss=2.252413304448128\n",
            "Step=15299 D_loss=0.6308238446712494, G_loss=2.044769179224968\n",
            "Step=15399 D_loss=0.6485325747728349, G_loss=3.1741505989432333\n",
            "Step=15499 D_loss=0.9907526224851609, G_loss=6.767712715864182\n",
            "Step=15599 D_loss=0.9724134343862535, G_loss=5.505983684360981\n",
            "Step=15699 D_loss=1.0068383860588073, G_loss=1.8149433469772338\n",
            "Step=15799 D_loss=0.6160565829277038, G_loss=3.181920823454857\n",
            "Step=15899 D_loss=0.8120953798294068, G_loss=1.8273298665881157\n",
            "Step=15999 D_loss=0.6888219821453094, G_loss=2.0930324026942255\n",
            "Step=16099 D_loss=0.565869054198265, G_loss=1.9168121182918547\n",
            "Step=16199 D_loss=0.6198731034994125, G_loss=2.678111974298954\n",
            "Step=16299 D_loss=0.5561860305070876, G_loss=3.090992833971977\n",
            "Step=16399 D_loss=0.7233174538612366, G_loss=5.7786946606636045\n",
            "Step=16499 D_loss=0.587652598619461, G_loss=2.299568256735802\n",
            "Step=16599 D_loss=0.4612780749797821, G_loss=2.572718300819397\n",
            "Step=16699 D_loss=0.44247196018695834, G_loss=2.9668867403268813\n",
            "Step=16799 D_loss=0.560571595430374, G_loss=2.3576438933610917\n",
            "Step=16899 D_loss=0.3572852981090546, G_loss=3.3435875505208967\n",
            "Step=16999 D_loss=0.6745946496725084, G_loss=2.3747770181298256\n",
            "Step=17099 D_loss=0.730600900053978, G_loss=2.5419135436415674\n",
            "Step=17199 D_loss=0.6352172178030013, G_loss=3.0760280713438988\n",
            "Step=17299 D_loss=0.6078084391355514, G_loss=2.7001006066799165\n",
            "Step=17399 D_loss=0.6245046794414522, G_loss=2.30496685564518\n",
            "Step=17499 D_loss=0.4631738871335984, G_loss=4.413909978270531\n",
            "Step=17599 D_loss=0.6528875070810318, G_loss=7.74806372910738\n",
            "Step=17699 D_loss=0.47374938011169443, G_loss=3.068416227400303\n",
            "Step=17799 D_loss=0.5842425906658172, G_loss=2.0173449322581294\n",
            "Step=17899 D_loss=0.59905353307724, G_loss=2.502788479924202\n",
            "Step=17999 D_loss=0.5076610165834428, G_loss=2.9311335679888724\n",
            "Step=18099 D_loss=0.5892541670799255, G_loss=2.39806821167469\n",
            "Step=18199 D_loss=0.683579534292221, G_loss=2.8094869151711466\n",
            "Step=18299 D_loss=0.7582578855752946, G_loss=2.6457199728488923\n",
            "Step=18399 D_loss=0.5566827076673507, G_loss=1.9456962817907333\n",
            "Step=18499 D_loss=0.49755338191986087, G_loss=2.1550949531793595\n",
            "Step=18599 D_loss=0.5580854791402817, G_loss=2.3889645162224773\n",
            "Step=18699 D_loss=0.6572297507524492, G_loss=2.1325462505221364\n",
            "Step=18799 D_loss=0.3852593976259231, G_loss=4.147700359523297\n",
            "Step=18899 D_loss=0.3372177028656007, G_loss=3.7082484596967693\n",
            "Step=18999 D_loss=0.6909744530916213, G_loss=2.3799093267321587\n",
            "Step=19099 D_loss=0.6114149224758147, G_loss=2.1525001519918443\n",
            "Step=19199 D_loss=0.49313064694404607, G_loss=2.1262646448612212\n",
            "Step=19299 D_loss=1.0009106040000915, G_loss=3.2055483689904216\n",
            "Step=19399 D_loss=0.7374969762563706, G_loss=5.88777731359005\n",
            "Step=19499 D_loss=0.6940076762437821, G_loss=3.2778255566954613\n",
            "Step=19599 D_loss=0.5035723400115968, G_loss=3.8189100092649464\n",
            "Step=19699 D_loss=0.5350745069980621, G_loss=3.652645148932934\n",
            "Step=19799 D_loss=0.5867349618673323, G_loss=3.5266442775726317\n",
            "Step=19899 D_loss=0.40904129207134243, G_loss=2.8301812660694123\n",
            "Step=19999 D_loss=0.4182679831981658, G_loss=2.6876386511325836\n",
            "Step=20099 D_loss=0.6213031923770903, G_loss=4.816173647940159\n",
            "Step=20199 D_loss=0.49182369232177736, G_loss=2.9910796257853507\n",
            "Step=20299 D_loss=0.6368937772512435, G_loss=2.5624365568161007\n",
            "Step=20399 D_loss=0.6164246553182601, G_loss=2.1148044899106027\n",
            "Step=20499 D_loss=0.3185188966989517, G_loss=3.3136643654108044\n",
            "Step=20599 D_loss=0.6237401515245437, G_loss=2.5098848688602446\n",
            "Step=20699 D_loss=0.5048858416080475, G_loss=2.7867703095078467\n",
            "Step=20799 D_loss=0.7514317142963409, G_loss=2.5667577642202377\n",
            "Step=20899 D_loss=0.31734658777713787, G_loss=3.928455822467804\n",
            "Step=20999 D_loss=0.5038487505912781, G_loss=4.641349849998951\n",
            "Step=21099 D_loss=0.45640475332736963, G_loss=5.236936594545841\n",
            "Step=21199 D_loss=0.6169965261220931, G_loss=2.565271329879761\n",
            "Step=21299 D_loss=0.6752498823404313, G_loss=2.243945027291775\n",
            "Step=21399 D_loss=0.45504644155502316, G_loss=4.157711270451546\n",
            "Step=21499 D_loss=0.32683465063571937, G_loss=4.388126349747181\n",
            "Step=21599 D_loss=0.4336597830057145, G_loss=7.747339231371879\n",
            "Step=21699 D_loss=0.56645891726017, G_loss=3.598793342709541\n",
            "Step=21799 D_loss=0.4439154511690139, G_loss=2.610655103325844\n",
            "Step=21899 D_loss=0.4490781331062317, G_loss=2.775213996171951\n",
            "Step=21999 D_loss=0.4338549661636354, G_loss=2.7960665634274484\n",
            "Step=22099 D_loss=0.505963346362114, G_loss=4.251866518557072\n",
            "Step=22199 D_loss=0.5725984299182892, G_loss=2.5259495824575424\n",
            "Step=22299 D_loss=0.6027594882249833, G_loss=2.260231727361679\n",
            "Step=22399 D_loss=0.5452521008253097, G_loss=1.9697043332457544\n",
            "Step=22499 D_loss=0.5190164572000503, G_loss=2.3932455652952194\n",
            "Step=22599 D_loss=0.484878153204918, G_loss=2.452392515540123\n",
            "Step=22699 D_loss=0.5029304659366609, G_loss=2.3043998816609386\n",
            "Step=22799 D_loss=0.5372366034984588, G_loss=4.816544167101383\n",
            "Step=22899 D_loss=0.45705944657325737, G_loss=3.3553814798593518\n",
            "Step=22999 D_loss=0.6942059254646302, G_loss=10.80503928810358\n",
            "Step=23099 D_loss=1.0489429706335067, G_loss=5.678993640244007\n",
            "Step=23199 D_loss=0.3425655710697173, G_loss=3.310877673327923\n",
            "Step=23299 D_loss=0.5448383063077927, G_loss=3.040423056781292\n",
            "Step=23399 D_loss=0.5112645798921586, G_loss=2.0549502858519553\n",
            "Step=23499 D_loss=0.41098413944244394, G_loss=2.4728751263022426\n",
            "Step=23599 D_loss=0.48275038778781887, G_loss=2.9249419456720354\n",
            "Step=23699 D_loss=0.3831381672620774, G_loss=3.290226019024849\n",
            "Step=23799 D_loss=0.37025717854499807, G_loss=3.996014542579651\n",
            "Step=23899 D_loss=0.5209352278709413, G_loss=3.551042644679546\n",
            "Step=23999 D_loss=0.40000445544719687, G_loss=3.911301762163639\n",
            "Step=24099 D_loss=0.4764654248952864, G_loss=4.030732840597629\n",
            "Step=24199 D_loss=0.385477288365364, G_loss=3.553113226890564\n",
            "Step=24299 D_loss=0.3874051290750502, G_loss=6.512951264977455\n",
            "Step=24399 D_loss=0.5421905475854873, G_loss=3.883444556593895\n",
            "Step=24499 D_loss=0.38530283927917486, G_loss=4.437585791349411\n",
            "Step=24599 D_loss=0.4969985079765321, G_loss=2.875904178619385\n",
            "Step=24699 D_loss=0.6863522076606752, G_loss=2.960490623116493\n",
            "Step=24799 D_loss=0.43397475898265847, G_loss=2.513155477941036\n",
            "Step=24899 D_loss=0.4737585735321044, G_loss=2.6238962325453756\n",
            "Step=24999 D_loss=0.3712440806627275, G_loss=2.418869251608849\n",
            "Step=25099 D_loss=0.4864305037260055, G_loss=2.26394749969244\n",
            "Step=25199 D_loss=0.43164788305759416, G_loss=2.7420326110720636\n",
            "Step=25299 D_loss=0.4826397997140883, G_loss=3.7820095625519756\n",
            "Step=25399 D_loss=0.6980324512720109, G_loss=2.314925521016121\n",
            "Step=25499 D_loss=0.5508348959684373, G_loss=2.5768745800852777\n",
            "Step=25599 D_loss=0.4072381836175919, G_loss=2.552962917387486\n",
            "Step=25699 D_loss=0.3945699429512024, G_loss=3.1252229219675067\n",
            "Step=25799 D_loss=0.5089238464832305, G_loss=4.152042277753353\n",
            "Step=25899 D_loss=0.739198128581047, G_loss=2.4873308345675467\n",
            "Step=25999 D_loss=0.6075988107919692, G_loss=2.296808089315891\n",
            "Step=26099 D_loss=0.5758512979745865, G_loss=2.4311502027511596\n",
            "Step=26199 D_loss=0.49082632422447214, G_loss=2.623581855893135\n",
            "Step=26299 D_loss=0.5012222355604171, G_loss=2.074899667799473\n",
            "Step=26399 D_loss=0.4312342208623887, G_loss=2.7915251082181927\n",
            "Step=26499 D_loss=0.3310298246145248, G_loss=4.869386147558689\n",
            "Step=26599 D_loss=0.23631564617156986, G_loss=4.830422994792461\n",
            "Step=26699 D_loss=0.22338072419166555, G_loss=4.670565451383591\n",
            "Step=26799 D_loss=0.40348869025707235, G_loss=4.7752830106019974\n",
            "Step=26899 D_loss=0.5279047536849976, G_loss=2.963503027558327\n",
            "Step=26999 D_loss=0.5929659795761109, G_loss=3.227342600226402\n",
            "Step=27099 D_loss=0.39170393705368045, G_loss=2.6116550505161285\n",
            "Step=27199 D_loss=0.4519714105129242, G_loss=2.9172342714667323\n",
            "Step=27299 D_loss=0.582462524175644, G_loss=3.010519195795059\n",
            "Step=27399 D_loss=0.39009832978248604, G_loss=3.308253191113472\n",
            "Step=27499 D_loss=0.42877525448799125, G_loss=6.261568837463855\n",
            "Step=27599 D_loss=0.8082135671377182, G_loss=3.6634010556340213\n",
            "Step=27699 D_loss=0.3803684544563293, G_loss=2.9919238647818567\n",
            "Step=27799 D_loss=0.6165654623508453, G_loss=2.8633460435271263\n",
            "Step=27899 D_loss=0.47806747794151316, G_loss=2.4026358342170715\n",
            "Step=27999 D_loss=0.6514236497879029, G_loss=2.659111577570438\n",
            "Step=28099 D_loss=0.4624763768911362, G_loss=2.8119160345196725\n",
            "Step=28199 D_loss=0.713865716457367, G_loss=2.715686376690864\n",
            "Step=28299 D_loss=0.5575662159919739, G_loss=2.2596039524674416\n",
            "Step=28399 D_loss=0.5554299288988114, G_loss=2.1421843439340593\n",
            "Step=28499 D_loss=0.5379711043834686, G_loss=2.5922267103195193\n",
            "Step=28599 D_loss=0.4840742051601411, G_loss=2.0414807283878327\n",
            "Step=28699 D_loss=0.36777659237384797, G_loss=2.2222056594491004\n",
            "Step=28799 D_loss=0.3839876651763916, G_loss=2.5938146832585334\n",
            "Step=28899 D_loss=0.39595058679580686, G_loss=2.928095741569996\n",
            "Step=28999 D_loss=0.7627660065889358, G_loss=3.6886586043238636\n",
            "Step=29099 D_loss=0.514684853553772, G_loss=3.4430728632211687\n",
            "Step=29199 D_loss=0.3623497700691223, G_loss=7.348595994710922\n",
            "Step=29299 D_loss=0.37764708459377294, G_loss=3.7754802200198174\n",
            "Step=29399 D_loss=0.4280823093652726, G_loss=3.2545467799901964\n",
            "Step=29499 D_loss=0.39535859465599055, G_loss=2.706039382815361\n",
            "Step=29599 D_loss=0.3550113499164581, G_loss=2.376046421825886\n",
            "Step=29699 D_loss=0.36350913941860197, G_loss=2.4710682684183123\n",
            "Step=29799 D_loss=0.5506886452436448, G_loss=2.7109219360351564\n",
            "Step=29899 D_loss=0.4347476220130919, G_loss=5.646036455631256\n",
            "Step=29999 D_loss=0.23195968985557558, G_loss=6.2084393325448035\n",
            "Step=30099 D_loss=0.24984149515628806, G_loss=4.7186116227507595\n",
            "Step=30199 D_loss=0.6834523981809617, G_loss=3.715864514410496\n",
            "Step=30299 D_loss=0.29256750762462624, G_loss=2.959725130498409\n",
            "Step=30399 D_loss=0.23745256006717685, G_loss=3.317893007099628\n",
            "Step=30499 D_loss=0.6324690437316894, G_loss=4.292795430123807\n",
            "Step=30599 D_loss=0.4517261260747909, G_loss=2.676068195104599\n",
            "Step=30699 D_loss=0.3743784689903259, G_loss=2.2047992330789565\n",
            "Step=30799 D_loss=0.3910876268148422, G_loss=2.989078302383423\n",
            "Step=30899 D_loss=0.2953017222881317, G_loss=3.066969383955002\n",
            "Step=30999 D_loss=0.37728740692138674, G_loss=2.2668693703413014\n",
            "Step=31099 D_loss=0.41176193118095405, G_loss=2.7352278032898902\n",
            "Step=31199 D_loss=0.28332591474056246, G_loss=2.7011636367440226\n",
            "Step=31299 D_loss=0.35352209866046913, G_loss=2.77726353764534\n",
            "Step=31399 D_loss=0.32324967682361605, G_loss=2.4288611805438993\n",
            "Step=31499 D_loss=0.39185341060161594, G_loss=2.7023201367259024\n",
            "Step=31599 D_loss=0.30491654753684994, G_loss=2.4295436435937883\n",
            "Step=31699 D_loss=0.3745494282245636, G_loss=2.6871075013279913\n",
            "Step=31799 D_loss=0.3104908514022827, G_loss=2.597875332832336\n",
            "Step=31899 D_loss=0.28828243017196653, G_loss=2.536766304373741\n",
            "Step=31999 D_loss=0.26270164847373956, G_loss=2.846654961705208\n",
            "Step=32099 D_loss=0.4002853786945343, G_loss=3.1267543789744376\n",
            "Step=32199 D_loss=0.32165692925453193, G_loss=2.931885350346565\n",
            "Step=32299 D_loss=0.36339957177639004, G_loss=2.6058706879615783\n",
            "Step=32399 D_loss=0.3275707888603211, G_loss=2.399445333778858\n",
            "Step=32499 D_loss=0.30144860029220577, G_loss=2.535014250278473\n",
            "Step=32599 D_loss=0.33999749600887297, G_loss=2.571219918727875\n",
            "Step=32699 D_loss=0.3412945514917374, G_loss=2.7628970456123354\n",
            "Step=32799 D_loss=0.351768810749054, G_loss=2.3140735799074172\n",
            "Step=32899 D_loss=0.3491457611322403, G_loss=2.6306867223978045\n",
            "Step=32999 D_loss=0.29955107390880586, G_loss=2.4527559047937393\n",
            "Step=33099 D_loss=0.2724506652355194, G_loss=2.541471920609474\n",
            "Step=33199 D_loss=0.27231793761253353, G_loss=2.7844757705926892\n",
            "Step=33299 D_loss=0.3254986003041267, G_loss=2.756539381444454\n",
            "Step=33399 D_loss=0.6093522131443023, G_loss=9.107227480113506\n",
            "Step=33499 D_loss=1.8031540066003802, G_loss=6.504307935833931\n",
            "Step=33599 D_loss=0.56468541264534, G_loss=4.137035262882709\n",
            "Step=33699 D_loss=0.5601350972056389, G_loss=5.360178517699241\n",
            "Step=33799 D_loss=0.6702700302004814, G_loss=3.3650427541136745\n",
            "Step=33899 D_loss=0.29535770088434216, G_loss=3.3756643664836883\n",
            "Step=33999 D_loss=0.31567726790905, G_loss=3.7674588659405712\n",
            "Step=34099 D_loss=0.27127140015363693, G_loss=4.352529245913028\n",
            "Step=34199 D_loss=0.4184068882465362, G_loss=5.639328002035618\n",
            "Step=34299 D_loss=0.4101590287685395, G_loss=3.704535711407661\n",
            "Step=34399 D_loss=0.3282448691129684, G_loss=3.432768669426441\n",
            "Step=34499 D_loss=0.34731662333011626, G_loss=2.6809879848361016\n",
            "Step=34599 D_loss=0.4269984751939774, G_loss=2.8038914144039153\n",
            "Step=34699 D_loss=0.39363435089588167, G_loss=2.724795923829079\n",
            "Step=34799 D_loss=0.3911850443482399, G_loss=3.0199161636829377\n",
            "Step=34899 D_loss=0.17348599374294277, G_loss=3.1363877558708193\n",
            "Step=34999 D_loss=0.18463322907686236, G_loss=4.352001713216304\n",
            "Step=35099 D_loss=0.4284674090147018, G_loss=3.7306896567344663\n",
            "Step=35199 D_loss=0.34662559628486633, G_loss=2.5573865923285486\n",
            "Step=35299 D_loss=0.28906389027833945, G_loss=2.5859336265921593\n",
            "Step=35399 D_loss=0.3807585656642914, G_loss=2.953764964044094\n",
            "Step=35499 D_loss=0.27142790496349334, G_loss=3.0036277854442597\n",
            "Step=35599 D_loss=0.3651851290464401, G_loss=2.992734707891941\n",
            "Step=35699 D_loss=0.7303784063458444, G_loss=5.907816816568374\n",
            "Step=35799 D_loss=0.5710098913311958, G_loss=8.150162741243838\n",
            "Step=35899 D_loss=0.2685602042078972, G_loss=3.6892646923661236\n",
            "Step=35999 D_loss=0.3238330432772636, G_loss=4.252780042886734\n",
            "Step=36099 D_loss=0.68887616366148, G_loss=3.776787396371365\n",
            "Step=36199 D_loss=0.4617781960964203, G_loss=2.5665698567032815\n",
            "Step=36299 D_loss=0.3304207986593246, G_loss=3.031825865507126\n",
            "Step=36399 D_loss=0.45764198213815693, G_loss=2.59645880907774\n",
            "Step=36499 D_loss=0.46454491674900056, G_loss=2.9041764310002325\n",
            "Step=36599 D_loss=0.5172703745961189, G_loss=2.8119838219881057\n",
            "Step=36699 D_loss=0.4598255556821823, G_loss=3.1756976360082625\n",
            "Step=36799 D_loss=0.30829621762037274, G_loss=2.5239637804031374\n",
            "Step=36899 D_loss=0.4349072307348251, G_loss=2.676864741444588\n",
            "Step=36999 D_loss=0.45136155098676684, G_loss=3.2056418663263324\n",
            "Step=37099 D_loss=0.15895114749670025, G_loss=4.899501722753048\n",
            "Step=37199 D_loss=0.4903368332982063, G_loss=3.0217006847262384\n",
            "Step=37299 D_loss=0.3416374742984772, G_loss=4.1725005358457565\n",
            "Step=37399 D_loss=0.12669154405593874, G_loss=6.1768849474191665\n",
            "Step=37499 D_loss=0.18501791656017302, G_loss=3.752217430472374\n",
            "Step=37599 D_loss=0.5349731478095054, G_loss=2.7053116655349734\n",
            "Step=37699 D_loss=0.39672154784202573, G_loss=2.6899654030799867\n",
            "Step=37799 D_loss=0.44977604180574415, G_loss=2.7410366478562356\n",
            "Step=37899 D_loss=0.479807371199131, G_loss=2.5446304062008855\n",
            "Step=37999 D_loss=0.5008919420838356, G_loss=2.5688446193933485\n",
            "Step=38099 D_loss=0.40468533545732494, G_loss=2.332402195930481\n",
            "Step=38199 D_loss=0.4087351980805397, G_loss=2.3218884736299517\n",
            "Step=38299 D_loss=0.3380486124753952, G_loss=2.2113429898023607\n",
            "Step=38399 D_loss=0.5659744790196419, G_loss=3.22178931593895\n",
            "Step=38499 D_loss=0.4112711411714554, G_loss=2.803789288699627\n",
            "Step=38599 D_loss=0.3904222220182419, G_loss=2.227283044457436\n",
            "Step=38699 D_loss=0.4998790016770363, G_loss=2.40299728423357\n",
            "Step=38799 D_loss=0.3788323494791984, G_loss=2.845339755117893\n",
            "Step=38899 D_loss=0.5741126900911331, G_loss=2.5080944824218747\n",
            "Step=38999 D_loss=0.40042336761951447, G_loss=2.235554254353046\n",
            "Step=39099 D_loss=0.3125652608275414, G_loss=2.311936379671097\n",
            "Step=39199 D_loss=0.2865571883320808, G_loss=2.5261794316768644\n",
            "Step=39299 D_loss=0.4181997492909431, G_loss=3.053420712351799\n",
            "Step=39399 D_loss=0.3554830950498581, G_loss=2.620978621840477\n",
            "Step=39499 D_loss=0.4889519408345223, G_loss=3.2314764937758444\n",
            "Step=39599 D_loss=0.47987356662750247, G_loss=3.6844295099377633\n",
            "Step=39699 D_loss=0.444671633541584, G_loss=2.229748784303665\n",
            "Step=39799 D_loss=0.512097693681717, G_loss=3.194109359383583\n",
            "Step=39899 D_loss=0.4244889539480209, G_loss=2.155678741335869\n",
            "Step=39999 D_loss=0.30480332434177404, G_loss=2.600155434012413\n",
            "Step=40099 D_loss=0.43117973536252974, G_loss=2.9883436414599416\n",
            "Step=40199 D_loss=0.3056026795506477, G_loss=4.449976400732994\n",
            "Step=40299 D_loss=0.4350099012255668, G_loss=2.6494551905989647\n",
            "Step=40399 D_loss=0.3835140272974968, G_loss=2.3304461428523062\n",
            "Step=40499 D_loss=0.3638027614355087, G_loss=2.748717493414879\n",
            "Step=40599 D_loss=0.38512753218412404, G_loss=2.692051558494568\n",
            "Step=40699 D_loss=0.4739894607663155, G_loss=3.033026893138885\n",
            "Step=40799 D_loss=0.34326204329729076, G_loss=2.275620231330395\n",
            "Step=40899 D_loss=0.3843968364596367, G_loss=2.3960337069630624\n",
            "Step=40999 D_loss=0.3398671412467957, G_loss=2.991396135985851\n",
            "Step=41099 D_loss=0.6224168533086778, G_loss=4.117341747879982\n",
            "Step=41199 D_loss=0.31751377820968635, G_loss=7.28624762326479\n",
            "Step=41299 D_loss=0.38912869244813925, G_loss=2.834162766635418\n",
            "Step=41399 D_loss=0.46898605763912204, G_loss=2.6297162687778473\n",
            "Step=41499 D_loss=0.3592861068248749, G_loss=2.4496054953336714\n",
            "Step=41599 D_loss=0.2936605823040009, G_loss=3.7820465353131296\n",
            "Step=41699 D_loss=0.28784160405397413, G_loss=3.567659371495247\n",
            "Step=41799 D_loss=0.34474951833486556, G_loss=2.6823444363474844\n",
            "Step=41899 D_loss=0.40379945635795594, G_loss=5.181417371034621\n",
            "Step=41999 D_loss=0.3602488738298416, G_loss=7.77325264930725\n",
            "Step=42099 D_loss=0.45385607600212097, G_loss=6.056251549124718\n",
            "Step=42199 D_loss=0.38688693910837174, G_loss=5.190085808038712\n",
            "Step=42299 D_loss=0.48337897866964336, G_loss=7.213999058306217\n",
            "Step=42399 D_loss=0.1204294216632843, G_loss=5.721656465530396\n",
            "Step=42499 D_loss=0.19636072576045993, G_loss=3.922992459833622\n",
            "Step=42599 D_loss=0.3430179479718208, G_loss=2.7162440928816793\n",
            "Step=42699 D_loss=0.3847048792243004, G_loss=2.887336911559105\n",
            "Step=42799 D_loss=0.3480437070131302, G_loss=2.9791324040293694\n",
            "Step=42899 D_loss=0.440369371175766, G_loss=3.6417348635196682\n",
            "Step=42999 D_loss=0.449567728638649, G_loss=3.6201530247926716\n",
            "Step=43099 D_loss=0.5865847733616829, G_loss=2.860075240731239\n",
            "Step=43199 D_loss=0.4222953945398331, G_loss=2.562744816243648\n",
            "Step=43299 D_loss=0.38117822349071506, G_loss=2.337532989680767\n",
            "Step=43399 D_loss=0.4103436574339867, G_loss=3.744268153905869\n",
            "Step=43499 D_loss=0.30348028570413593, G_loss=3.3700712525844576\n",
            "Step=43599 D_loss=0.43891751796007156, G_loss=2.5062887677550316\n",
            "Step=43699 D_loss=0.3206254237890244, G_loss=3.577810387611389\n",
            "Step=43799 D_loss=0.320789321064949, G_loss=5.207808078527451\n",
            "Step=43899 D_loss=0.4078846284747124, G_loss=3.1886076498031617\n",
            "Step=43999 D_loss=0.49817219883203506, G_loss=2.3729438638687133\n",
            "Step=44099 D_loss=0.3115901747345924, G_loss=2.4717223620414734\n",
            "Step=44199 D_loss=0.307895182967186, G_loss=2.787951283156872\n",
            "Step=44299 D_loss=0.3444652664661407, G_loss=2.9765256407856944\n",
            "Step=44399 D_loss=0.3359111377596855, G_loss=3.0021320468187334\n",
            "Step=44499 D_loss=0.26091248422861096, G_loss=3.7256093278527262\n",
            "Step=44599 D_loss=0.3233289214968681, G_loss=3.1735471951961514\n",
            "Step=44699 D_loss=0.3378763702511788, G_loss=2.6245777520537374\n",
            "Step=44799 D_loss=0.22201285868883125, G_loss=3.2044078782200813\n",
            "Step=44899 D_loss=0.40092692166566846, G_loss=3.4524482503533367\n",
            "Step=44999 D_loss=0.28358288198709486, G_loss=2.6094845202565193\n",
            "Step=45099 D_loss=0.5008339759707451, G_loss=3.3056443026661873\n",
            "Step=45199 D_loss=0.37402486860752104, G_loss=3.2355701979994778\n",
            "Step=45299 D_loss=0.6152138671278953, G_loss=6.946357074081898\n",
            "Step=45399 D_loss=0.5973867627978324, G_loss=2.4685215973854064\n",
            "Step=45499 D_loss=0.28564275383949284, G_loss=8.08921215891838\n",
            "Step=45599 D_loss=0.4664010661840439, G_loss=4.001251967847347\n",
            "Step=45699 D_loss=0.34061029493808753, G_loss=5.478173450827598\n",
            "Step=45799 D_loss=0.24702083140611647, G_loss=5.720595176517964\n",
            "Step=45899 D_loss=0.1739104986190796, G_loss=4.047314174473286\n",
            "Step=45999 D_loss=0.21056429952383038, G_loss=5.045887055695057\n",
            "Step=46099 D_loss=0.5892099645733833, G_loss=3.4686727222800258\n",
            "Step=46199 D_loss=0.39515210688114166, G_loss=2.497789450287819\n",
            "Step=46299 D_loss=0.4357978755235672, G_loss=2.7370970377326014\n",
            "Step=46399 D_loss=0.3797936901450157, G_loss=3.500492852628231\n",
            "Step=46499 D_loss=0.25317927449941635, G_loss=4.1584365209937095\n",
            "Step=46599 D_loss=0.39104707539081573, G_loss=2.813772529363632\n",
            "Step=46699 D_loss=0.3018954747915268, G_loss=3.0429488459229472\n",
            "Step=46799 D_loss=0.478409438431263, G_loss=3.5734560102224355\n",
            "Step=46899 D_loss=0.4411577156186104, G_loss=2.206711188852787\n",
            "Step=46999 D_loss=0.3270706680417061, G_loss=2.6354098466038702\n",
            "Step=47099 D_loss=0.5050979697704316, G_loss=3.01936909109354\n",
            "Step=47199 D_loss=0.2907247424125672, G_loss=3.479312395751476\n",
            "Step=47299 D_loss=0.2082653242349624, G_loss=3.409715411365032\n",
            "Step=47399 D_loss=0.29135350346565253, G_loss=2.449346983730793\n",
            "Step=47499 D_loss=0.25371448993682855, G_loss=2.7445517134666444\n",
            "Step=47599 D_loss=0.2978490072488785, G_loss=2.835949892401695\n",
            "Step=47699 D_loss=0.5138820028305053, G_loss=3.3985364201664927\n",
            "Step=47799 D_loss=0.3766994112730026, G_loss=2.9179584541916848\n",
            "Step=47899 D_loss=0.3709489187598228, G_loss=2.2446783253550526\n",
            "Step=47999 D_loss=0.24423309087753298, G_loss=2.6241898417472838\n",
            "Step=48099 D_loss=0.2328041222691536, G_loss=2.817670939564705\n",
            "Step=48199 D_loss=0.29385763674974436, G_loss=3.0407862958312033\n",
            "Step=48299 D_loss=0.25499907642602926, G_loss=3.143700375854969\n",
            "Step=48399 D_loss=0.3384098109602928, G_loss=4.020186680853366\n",
            "Step=48499 D_loss=0.2610243663191796, G_loss=4.161421736180783\n",
            "Step=48599 D_loss=0.32514235109090805, G_loss=7.420841481983661\n",
            "Step=48699 D_loss=0.22717304885387424, G_loss=5.684319389760494\n",
            "Step=48799 D_loss=0.14473483949899674, G_loss=7.603953989744186\n",
            "Step=48899 D_loss=0.25347207129001614, G_loss=5.659293821752072\n",
            "Step=48999 D_loss=0.2668397119641305, G_loss=3.666217505633831\n",
            "Step=49099 D_loss=0.40974507927894593, G_loss=2.794580374062061\n",
            "Step=49199 D_loss=0.3328255337476731, G_loss=2.6964823150634767\n",
            "Step=49299 D_loss=0.27534040421247485, G_loss=2.8551368197798728\n",
            "Step=49399 D_loss=0.25285958349704746, G_loss=2.899277057349682\n",
            "Step=49499 D_loss=0.18426709502935407, G_loss=3.1807301139831545\n",
            "Step=49599 D_loss=0.1700328269600868, G_loss=3.2432318407297136\n",
            "Step=49699 D_loss=0.27479180127382274, G_loss=3.530439486205578\n",
            "Step=49799 D_loss=0.4392888513207436, G_loss=4.208521167635917\n",
            "Step=49899 D_loss=0.3264484187960625, G_loss=2.565944028198719\n",
            "Step=49999 D_loss=0.24880570799112323, G_loss=2.8641281035542487\n",
            "Reached 50001 epochs for GAN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo3zlTrsma4B"
      },
      "source": [
        "#initializing cnn\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import os\r\n",
        "\r\n",
        "SEED = 42\r\n",
        "tf.set_random_seed(SEED)\r\n",
        "class CNN():\r\n",
        "\r\n",
        "    def __init__(self, num_features, num_historical_days, is_train=True):\r\n",
        "      \r\n",
        "        self.X = tf.placeholder(tf.float32, shape=[None, num_historical_days, num_features])\r\n",
        "        X = tf.reshape(self.X, [-1, num_historical_days, 1, num_features])\r\n",
        "        self.Y = tf.placeholder(tf.int32, shape=[None, 2])\r\n",
        "        self.keep_prob = tf.placeholder(tf.float32, shape=[])\r\n",
        "\r\n",
        "        with tf.variable_scope(\"cnn\"):\r\n",
        "            #[filter_height, filter_width, in_channels, out_channels]\r\n",
        "            k1 = tf.Variable(tf.truncated_normal([3, 1, num_features, 16],\r\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\r\n",
        "            b1 = tf.Variable(tf.zeros([16], dtype=tf.float32))\r\n",
        "\r\n",
        "            conv = tf.nn.conv2d(X,k1,strides=[1, 1, 1, 1],padding='SAME')\r\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b1))\r\n",
        "            if is_train:\r\n",
        "                relu = tf.nn.dropout(relu, keep_prob = self.keep_prob)\r\n",
        "            print(relu)\r\n",
        "\r\n",
        "\r\n",
        "            k2 = tf.Variable(tf.truncated_normal([3, 1, 16, 32],\r\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\r\n",
        "            b2 = tf.Variable(tf.zeros([32], dtype=tf.float32))\r\n",
        "            conv = tf.nn.conv2d(relu, k2,strides=[1, 1, 1, 1],padding='SAME')\r\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b2))\r\n",
        "            if is_train:\r\n",
        "                relu = tf.nn.dropout(relu, keep_prob = self.keep_prob)\r\n",
        "            print(relu)\r\n",
        "\r\n",
        "\r\n",
        "            k3 = tf.Variable(tf.truncated_normal([3, 1, 32, 64],\r\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\r\n",
        "            b3 = tf.Variable(tf.zeros([64], dtype=tf.float32))\r\n",
        "            conv = tf.nn.conv2d(relu, k3, strides=[1, 1, 1, 1], padding='VALID')\r\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b3))\r\n",
        "            if is_train:\r\n",
        "                relu = tf.nn.dropout(relu, keep_prob=self.keep_prob)\r\n",
        "            print(relu)\r\n",
        "\r\n",
        "\r\n",
        "            flattened_convolution_size = int(relu.shape[1]) * int(relu.shape[2]) * int(relu.shape[3])\r\n",
        "            print(flattened_convolution_size)\r\n",
        "            flattened_convolution = features = tf.reshape(relu, [-1, flattened_convolution_size])\r\n",
        "\r\n",
        "            if is_train:\r\n",
        "                flattened_convolution =  tf.nn.dropout(flattened_convolution, keep_prob=self.keep_prob)\r\n",
        "\r\n",
        "            W1 = tf.Variable(tf.truncated_normal([18*1*64, 32]))\r\n",
        "            b4 = tf.Variable(tf.truncated_normal([32]))\r\n",
        "            h1 = tf.nn.relu(tf.matmul(flattened_convolution, W1) + b4)\r\n",
        "\r\n",
        "\r\n",
        "            W2 = tf.Variable(tf.truncated_normal([32, 2]))\r\n",
        "            logits = tf.matmul(h1, W2)\r\n",
        "\r\n",
        "            #self.accuracy = tf.metrics.accuracy(tf.argmax(self.Y, 1), tf.argmax(logits, 1))\r\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.Y, 1), tf.argmax(logits, 1)), tf.float32))\r\n",
        "            self.confusion_matrix = tf.confusion_matrix(tf.argmax(self.Y, 1), tf.argmax(logits, 1))\r\n",
        "            tf.summary.scalar('accuracy', self.accuracy)\r\n",
        "            theta_D = [k1, b1, k2, b2, k3, b3, W1, b4, W2]           \r\n",
        "            \r\n",
        "            # D_prob = tf.nn.sigmoid(D_logit)\r\n",
        "\r\n",
        "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=logits))\r\n",
        "        tf.summary.scalar('loss', self.loss)\r\n",
        "        # self.D_l2_loss = (0.0001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_D]) / len(theta_D))\r\n",
        "        # self.D_loss = D_loss_real + D_loss_fake + self.D_l2_loss\r\n",
        "        # self.G_l2_loss = (0.00001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_G]) / len(theta_G))\r\n",
        "        # self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) + self.G_l2_loss\r\n",
        "\r\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.loss)\r\n",
        "        self.summary = tf.summary.merge_all()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XCdv9X0Pm3Sw",
        "outputId": "766de2bc-5369-4524-adb8-7e4230b76b4f"
      },
      "source": [
        "#training cnn\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "# from cnn import CNN\r\n",
        "import random\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n",
        "import xgboost as xgb\r\n",
        "from sklearn.externals import joblib\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "#from plot_confusion_matrix import plot_confusion_matrix\r\n",
        "#from plot_confusion_matrix import plot_confusion_matrix\r\n",
        "\r\n",
        "random.seed(42)\r\n",
        "\r\n",
        "class TrainCNN:\r\n",
        "\r\n",
        "    def __init__(self, num_historical_days, days=10, pct_change=0):\r\n",
        "        self.data = []\r\n",
        "        self.labels = []\r\n",
        "        self.test_data = []\r\n",
        "        self.test_labels = []\r\n",
        "        self.cnn = CNN(num_features=5, num_historical_days=num_historical_days, is_train=False)\r\n",
        "#         files = [os.path.join('./stock_data', f) for f in os.listdir('./stock_data')]\r\n",
        "\r\n",
        "        # Google Drive Method\r\n",
        "        files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")]\r\n",
        "#         print(files)\r\n",
        "    \r\n",
        "    \r\n",
        "        for file in files:\r\n",
        "            print(file)\r\n",
        "            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\r\n",
        "            df = df[['open','high','low','close','volume']]\r\n",
        "            # data for new column labels that will use the pct_change of the closing data.\r\n",
        "            # pct_change measure change between current and prior element. Map these into a 1x2\r\n",
        "            # array to show if the pct_change > (our desired threshold) or less than.\r\n",
        "            labels = df.close.pct_change(days).map(lambda x: [int(x > pct_change/100.0), int(x <= pct_change/100.0)])\r\n",
        "            \r\n",
        "            # rolling normalization. (df - df.mean) / (df.max - df.min)\r\n",
        "            df = ((df -\r\n",
        "            df.rolling(num_historical_days).mean().shift(-num_historical_days))\r\n",
        "            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\r\n",
        "            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\r\n",
        "            df['labels'] = labels\r\n",
        "\r\n",
        "            # doing pct_change will give some rows (like first row) a NaN value. Drop that.\r\n",
        "            df = df.dropna()\r\n",
        "\r\n",
        "            # Do the testing data split\r\n",
        "            test_df = df[:365]\r\n",
        "            df = df[400:]\r\n",
        "\r\n",
        "            # get the predictors of the dataframe\r\n",
        "            data = df[['open','high','low','close','volume']].values\r\n",
        "\r\n",
        "            # the response value\r\n",
        "            labels = df['labels'].values\r\n",
        "\r\n",
        "            # start at num_historical_days and iterate the full length of the training\r\n",
        "            # data at intervals of num_historical_days\r\n",
        "            for i in range(num_historical_days, len(df), num_historical_days):\r\n",
        "                # split the df into arrays of length num_historical_days and append\r\n",
        "                # to data, i.e. array of df[curr - num_days : curr] -> a batch of values\r\n",
        "                self.data.append(data[i-num_historical_days:i])\r\n",
        "\r\n",
        "                # appending if price went up or down in curr day of \"i\" we are looking\r\n",
        "                # at\r\n",
        "                self.labels.append(labels[i-1])\r\n",
        "            \r\n",
        "            # do same for test data\r\n",
        "            data = test_df[['open','high','low','close','volume']].values\r\n",
        "            labels = test_df['labels'].values\r\n",
        "            for i in range(num_historical_days, len(test_df), 1):\r\n",
        "                self.test_data.append(data[i-num_historical_days:i])\r\n",
        "                self.test_labels.append(labels[i-1])\r\n",
        "\r\n",
        "    # a function to get a random_batch of data.\r\n",
        "    def random_batch(self, batch_size=128):\r\n",
        "        batch = []\r\n",
        "        labels = []\r\n",
        "        # zip concatenates each array index of both arrays together\r\n",
        "        data = list(zip(self.data, self.labels))\r\n",
        "        i = 0\r\n",
        "        while True:\r\n",
        "            i+= 1\r\n",
        "            while True:\r\n",
        "                # pick a random array, i.e. range of days, from data\r\n",
        "                d = random.choice(data)\r\n",
        "                # balance the data with equal number of positive pct_change\r\n",
        "                # and negative pct_change\r\n",
        "                if(d[1][0]== int(i%2)):\r\n",
        "                    break\r\n",
        "            batch.append(d[0])  # append the range of days we got to batch\r\n",
        "            labels.append(d[1])  # append the label of that range of data we got\r\n",
        "            if (len(batch) == batch_size):\r\n",
        "                yield batch, labels\r\n",
        "                batch = []\r\n",
        "                labels = []\r\n",
        "\r\n",
        "    def train(self, print_steps=100, display_steps=100, save_steps=SAVE_STEPS_AMOUNT, batch_size=128, keep_prob=0.6):\r\n",
        "        if not os.path.exists(f'{googlepath}cnn_models'):\r\n",
        "            os.makedirs(f'{googlepath}cnn_models')\r\n",
        "        if not os.path.exists(f'{googlepath}logs'):\r\n",
        "            os.makedirs(f'{googlepath}logs')\r\n",
        "        if os.path.exists(f'{googlepath}logs/train'):\r\n",
        "            for file in [os.path.join(f'{googlepath}logs/train/', f) for f in os.listdir(f'{googlepath}logs/train/')]:\r\n",
        "                os.remove(file)\r\n",
        "        if os.path.exists(f'{googlepath}logs/test'):\r\n",
        "            for file in [os.path.join(f'{googlepath}logs/test/', f) for f in os.listdir(f'{googlepath}logs/test')]:\r\n",
        "                os.remove(file)\r\n",
        "\r\n",
        "        sess = tf.Session()\r\n",
        "        loss = 0\r\n",
        "        l2_loss = 0\r\n",
        "        accuracy = 0\r\n",
        "        saver = tf.train.Saver()\r\n",
        "        train_writer = tf.summary.FileWriter(f'{googlepath}/logs/train')\r\n",
        "        test_writer = tf.summary.FileWriter(f'{googlepath}/logs/test')\r\n",
        "        sess.run(tf.global_variables_initializer())\r\n",
        "        \r\n",
        "        test_loss_array = []\r\n",
        "        test_accuracy_array = []\r\n",
        "        currentStep = \"0\"\r\n",
        "        \r\n",
        "        if os.path.exists(f'{googlepath}cnn_models/checkpoint'):\r\n",
        "                with open(f'{googlepath}cnn_models/checkpoint', 'rb') as f:\r\n",
        "                    model_name = next(f).split('\"'.encode())[1]\r\n",
        "                filename = \"{}cnn_models/{}\".format(googlepath, model_name.decode())\r\n",
        "                currentStep = filename.split(\"-\")[1]\r\n",
        "                new_saver = tf.train.import_meta_graph('{}.meta'.format(filename))\r\n",
        "                new_saver.restore(sess, \"{}\".format(filename))\r\n",
        "\r\n",
        "        for i, [X, y] in enumerate(self.random_batch(batch_size)):\r\n",
        "\r\n",
        "          \r\n",
        "            _, loss_curr, accuracy_curr = sess.run([self.cnn.optimizer, self.cnn.loss, self.cnn.accuracy], feed_dict=\r\n",
        "                    {self.cnn.X:X, self.cnn.Y:y, self.cnn.keep_prob:keep_prob})\r\n",
        "            loss += loss_curr\r\n",
        "            accuracy += accuracy_curr\r\n",
        "            if (i+1) % print_steps == 0:\r\n",
        "                print('Step={} loss={}, accuracy={}'.format(i + int(currentStep), loss/print_steps, accuracy/print_steps))\r\n",
        "                loss = 0\r\n",
        "                l2_loss = 0\r\n",
        "                accuracy = 0\r\n",
        "                test_loss, test_accuracy, confusion_matrix = sess.run([self.cnn.loss, self.cnn.accuracy, self.cnn.confusion_matrix], feed_dict={self.cnn.X:self.test_data, self.cnn.Y:self.test_labels, self.cnn.keep_prob:1})\r\n",
        "                test_loss_array.append(test_loss)\r\n",
        "                test_accuracy_array.append(test_accuracy)\r\n",
        "                print(\"Test loss = {}, Test accuracy = {}\".format(test_loss, test_accuracy))\r\n",
        "            if (i+1) % save_steps == 0:\r\n",
        "                saver.save(sess,  f'{googlepath}cnn_models/cnn.ckpt', i)\r\n",
        "\r\n",
        "            if (i+1) % display_steps == 0:\r\n",
        "                summary = sess.run(self.cnn.summary, feed_dict=\r\n",
        "                    {self.cnn.X:X, self.cnn.Y:y, self.cnn.keep_prob:keep_prob})\r\n",
        "                train_writer.add_summary(summary, i)\r\n",
        "                summary = sess.run(self.cnn.summary, feed_dict={\r\n",
        "                    self.cnn.X:self.test_data, self.cnn.Y:self.test_labels, self.cnn.keep_prob:1})\r\n",
        "                test_writer.add_summary(summary, i)\r\n",
        "            \r\n",
        "            # end training at training_amount epochs\r\n",
        "            if (i + int(currentStep)) > TRAINING_AMOUNT:\r\n",
        "                print(\"Reached {} epochs for CNN\".format(i + int(currentStep)))\r\n",
        "                sess.close()\r\n",
        "                #print(confusion_matrix)\r\n",
        "                #plot_confusion_matrix(confusion_matrix, ['Down', 'Up'], normalize=True, title=\"CNN Confusion Matrix\")\r\n",
        "                \r\n",
        "                axisX = np.arange(0,len(test_loss_array),1)\r\n",
        "                plt.plot(axisX, test_loss_array, label='test loss')\r\n",
        "                plt.plot(axisX, test_accuracy_array, label='test accuracy')\r\n",
        "                plt.legend()\r\n",
        "                plt.title('test loss and accuracy')\r\n",
        "                plt.show()\r\n",
        "\r\n",
        "                break\r\n",
        "\r\n",
        "\r\n",
        "# if __name__ == '__main__':\r\n",
        "tf.reset_default_graph()\r\n",
        "cnn = TrainCNN(num_historical_days=HISTORICAL_DAYS_AMOUNT, days=DAYS_AHEAD, pct_change=PCT_CHANGE_AMOUNT)\r\n",
        "cnn.train()\r\n",
        "#print(confusion_matrix)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"cnn/Relu:0\", shape=(?, 20, 1, 16), dtype=float32)\n",
            "Tensor(\"cnn/Relu_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"cnn/Relu_2:0\", shape=(?, 18, 1, 64), dtype=float32)\n",
            "1152\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BTC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/LTC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/NMC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/PPC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/ETH.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BNB.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/DOT.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/ADA.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/USDT.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XRP.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/LINK.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BCH.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XLM.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/UNI.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/USDC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/DOGE.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/WBTC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/AAVE.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/ATOM.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/EOS.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XEM.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XMR.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BSV.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/TRX.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/HT .csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/MIOTA.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XTZ.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/VET.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/THETA.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/CRO.csv\n",
            "Step=99 loss=0.2888705164939165, accuracy=0.913203125\n",
            "Test loss = 1.0755513906478882, Test accuracy = 0.5275362133979797\n",
            "Step=199 loss=0.046483664512634276, accuracy=1.0\n",
            "Test loss = 1.158915638923645, Test accuracy = 0.5362318754196167\n",
            "Step=299 loss=0.01743388161994517, accuracy=1.0\n",
            "Test loss = 1.213747501373291, Test accuracy = 0.5449275374412537\n",
            "Step=399 loss=0.009652790864929556, accuracy=1.0\n",
            "Test loss = 1.2624163627624512, Test accuracy = 0.5362318754196167\n",
            "Step=499 loss=0.006233140374533832, accuracy=1.0\n",
            "Test loss = 1.3006374835968018, Test accuracy = 0.539130449295044\n",
            "Step=599 loss=0.004335538493469357, accuracy=1.0\n",
            "Test loss = 1.3393332958221436, Test accuracy = 0.539130449295044\n",
            "Step=699 loss=0.0030725276027806104, accuracy=1.0\n",
            "Test loss = 1.3697749376296997, Test accuracy = 0.5420289635658264\n",
            "Step=799 loss=0.002310882518067956, accuracy=1.0\n",
            "Test loss = 1.3965802192687988, Test accuracy = 0.5478261113166809\n",
            "Step=899 loss=0.0018274927022866904, accuracy=1.0\n",
            "Test loss = 1.4224690198898315, Test accuracy = 0.5420289635658264\n",
            "Step=999 loss=0.001447717578848824, accuracy=1.0\n",
            "Test loss = 1.4460970163345337, Test accuracy = 0.5420289635658264\n",
            "Step=1099 loss=0.001185727223055437, accuracy=1.0\n",
            "Test loss = 1.4703824520111084, Test accuracy = 0.5420289635658264\n",
            "Step=1199 loss=0.0009751492709619925, accuracy=1.0\n",
            "Test loss = 1.4919304847717285, Test accuracy = 0.539130449295044\n",
            "Step=1299 loss=0.0008145166025497019, accuracy=1.0\n",
            "Test loss = 1.512319803237915, Test accuracy = 0.5333333611488342\n",
            "Step=1399 loss=0.0006741713965311647, accuracy=1.0\n",
            "Test loss = 1.5322213172912598, Test accuracy = 0.539130449295044\n",
            "Step=1499 loss=0.000564191515732091, accuracy=1.0\n",
            "Test loss = 1.5519390106201172, Test accuracy = 0.5449275374412537\n",
            "Step=1599 loss=0.0004881299432599917, accuracy=1.0\n",
            "Test loss = 1.5712275505065918, Test accuracy = 0.5449275374412537\n",
            "Step=1699 loss=0.00041662859002826734, accuracy=1.0\n",
            "Test loss = 1.5893847942352295, Test accuracy = 0.5420289635658264\n",
            "Step=1799 loss=0.0003603862976888195, accuracy=1.0\n",
            "Test loss = 1.6067949533462524, Test accuracy = 0.5420289635658264\n",
            "Step=1899 loss=0.0003156731271883473, accuracy=1.0\n",
            "Test loss = 1.6229034662246704, Test accuracy = 0.539130449295044\n",
            "Step=1999 loss=0.0002777606315794401, accuracy=1.0\n",
            "Test loss = 1.6380717754364014, Test accuracy = 0.539130449295044\n",
            "Step=2099 loss=0.000245184177038027, accuracy=1.0\n",
            "Test loss = 1.652650237083435, Test accuracy = 0.5420289635658264\n",
            "Step=2199 loss=0.00021891667784075254, accuracy=1.0\n",
            "Test loss = 1.667502999305725, Test accuracy = 0.5449275374412537\n",
            "Step=2299 loss=0.00019531934973201714, accuracy=1.0\n",
            "Test loss = 1.6819093227386475, Test accuracy = 0.5478261113166809\n",
            "Step=2399 loss=0.00017301508560194634, accuracy=1.0\n",
            "Test loss = 1.6955760717391968, Test accuracy = 0.5507246255874634\n",
            "Step=2499 loss=0.00015565133682684973, accuracy=1.0\n",
            "Test loss = 1.7093406915664673, Test accuracy = 0.5507246255874634\n",
            "Step=2599 loss=0.00013959730495116673, accuracy=1.0\n",
            "Test loss = 1.7235143184661865, Test accuracy = 0.5478261113166809\n",
            "Step=2699 loss=0.00012636708735954015, accuracy=1.0\n",
            "Test loss = 1.7375341653823853, Test accuracy = 0.5478261113166809\n",
            "Step=2799 loss=0.0001142975300172111, accuracy=1.0\n",
            "Test loss = 1.7514089345932007, Test accuracy = 0.5420289635658264\n",
            "Step=2899 loss=0.0001036331142677227, accuracy=1.0\n",
            "Test loss = 1.7644898891448975, Test accuracy = 0.5449275374412537\n",
            "Step=2999 loss=9.44266701117158e-05, accuracy=1.0\n",
            "Test loss = 1.7771316766738892, Test accuracy = 0.5449275374412537\n",
            "Step=3099 loss=8.56716456473805e-05, accuracy=1.0\n",
            "Test loss = 1.7896981239318848, Test accuracy = 0.5449275374412537\n",
            "Step=3199 loss=7.802062595146708e-05, accuracy=1.0\n",
            "Test loss = 1.8023712635040283, Test accuracy = 0.5449275374412537\n",
            "Step=3299 loss=7.123333380150143e-05, accuracy=1.0\n",
            "Test loss = 1.815342664718628, Test accuracy = 0.5420289635658264\n",
            "Step=3399 loss=6.545769010699587e-05, accuracy=1.0\n",
            "Test loss = 1.8291798830032349, Test accuracy = 0.5420289635658264\n",
            "Step=3499 loss=5.936503264820203e-05, accuracy=1.0\n",
            "Test loss = 1.8434369564056396, Test accuracy = 0.5420289635658264\n",
            "Step=3599 loss=5.4198471698327925e-05, accuracy=1.0\n",
            "Test loss = 1.8578053712844849, Test accuracy = 0.5449275374412537\n",
            "Step=3699 loss=4.8842956239241176e-05, accuracy=1.0\n",
            "Test loss = 1.871423602104187, Test accuracy = 0.5449275374412537\n",
            "Step=3799 loss=4.460860352992313e-05, accuracy=1.0\n",
            "Test loss = 1.8851414918899536, Test accuracy = 0.5420289635658264\n",
            "Step=3899 loss=4.121256144571817e-05, accuracy=1.0\n",
            "Test loss = 1.8989349603652954, Test accuracy = 0.5420289635658264\n",
            "Step=3999 loss=3.769847280636896e-05, accuracy=1.0\n",
            "Test loss = 1.9126149415969849, Test accuracy = 0.5449275374412537\n",
            "Step=4099 loss=3.5181223047402455e-05, accuracy=1.0\n",
            "Test loss = 1.926185131072998, Test accuracy = 0.5449275374412537\n",
            "Step=4199 loss=3.224002079150523e-05, accuracy=1.0\n",
            "Test loss = 1.9398809671401978, Test accuracy = 0.5420289635658264\n",
            "Step=4299 loss=2.9510620424844092e-05, accuracy=1.0\n",
            "Test loss = 1.9537367820739746, Test accuracy = 0.5420289635658264\n",
            "Step=4399 loss=2.7318310840200867e-05, accuracy=1.0\n",
            "Test loss = 1.967379093170166, Test accuracy = 0.5420289635658264\n",
            "Step=4499 loss=2.50862205393787e-05, accuracy=1.0\n",
            "Test loss = 1.9822750091552734, Test accuracy = 0.5420289635658264\n",
            "Step=4599 loss=2.3325908496190096e-05, accuracy=1.0\n",
            "Test loss = 1.9961061477661133, Test accuracy = 0.5420289635658264\n",
            "Step=4699 loss=2.1538437213166617e-05, accuracy=1.0\n",
            "Test loss = 2.0092852115631104, Test accuracy = 0.5420289635658264\n",
            "Step=4799 loss=1.9916350120183778e-05, accuracy=1.0\n",
            "Test loss = 2.022702217102051, Test accuracy = 0.5420289635658264\n",
            "Step=4899 loss=1.8218042241642253e-05, accuracy=1.0\n",
            "Test loss = 2.0347330570220947, Test accuracy = 0.5420289635658264\n",
            "Step=4999 loss=1.6999898998619757e-05, accuracy=1.0\n",
            "Test loss = 2.0471718311309814, Test accuracy = 0.5420289635658264\n",
            "Step=5099 loss=1.575126529132831e-05, accuracy=1.0\n",
            "Test loss = 2.0593020915985107, Test accuracy = 0.5420289635658264\n",
            "Step=5199 loss=1.4591866092814598e-05, accuracy=1.0\n",
            "Test loss = 2.0718533992767334, Test accuracy = 0.5420289635658264\n",
            "Step=5299 loss=1.3533139945138828e-05, accuracy=1.0\n",
            "Test loss = 2.0842971801757812, Test accuracy = 0.5420289635658264\n",
            "Step=5399 loss=1.2627695505216251e-05, accuracy=1.0\n",
            "Test loss = 2.0978896617889404, Test accuracy = 0.5449275374412537\n",
            "Step=5499 loss=1.1682716676659766e-05, accuracy=1.0\n",
            "Test loss = 2.1106812953948975, Test accuracy = 0.5449275374412537\n",
            "Step=5599 loss=1.0828268677869346e-05, accuracy=1.0\n",
            "Test loss = 2.1219112873077393, Test accuracy = 0.5449275374412537\n",
            "Step=5699 loss=1.0035370260084164e-05, accuracy=1.0\n",
            "Test loss = 2.1323633193969727, Test accuracy = 0.5420289635658264\n",
            "Step=5799 loss=9.290256202802994e-06, accuracy=1.0\n",
            "Test loss = 2.143536329269409, Test accuracy = 0.5420289635658264\n",
            "Step=5899 loss=8.630598890704278e-06, accuracy=1.0\n",
            "Test loss = 2.154733657836914, Test accuracy = 0.5420289635658264\n",
            "Step=5999 loss=8.032622413338685e-06, accuracy=1.0\n",
            "Test loss = 2.165911912918091, Test accuracy = 0.5420289635658264\n",
            "Step=6099 loss=7.464000182153541e-06, accuracy=1.0\n",
            "Test loss = 2.176471471786499, Test accuracy = 0.5420289635658264\n",
            "Step=6199 loss=7.071395320963347e-06, accuracy=1.0\n",
            "Test loss = 2.187126636505127, Test accuracy = 0.5420289635658264\n",
            "Step=6299 loss=6.555233221661183e-06, accuracy=1.0\n",
            "Test loss = 2.1979010105133057, Test accuracy = 0.5420289635658264\n",
            "Step=6399 loss=6.106880077823007e-06, accuracy=1.0\n",
            "Test loss = 2.2084128856658936, Test accuracy = 0.5420289635658264\n",
            "Step=6499 loss=5.6816228607203815e-06, accuracy=1.0\n",
            "Test loss = 2.2189974784851074, Test accuracy = 0.5420289635658264\n",
            "Step=6599 loss=5.3019160441181155e-06, accuracy=1.0\n",
            "Test loss = 2.230304479598999, Test accuracy = 0.5420289635658264\n",
            "Step=6699 loss=4.931280309392605e-06, accuracy=1.0\n",
            "Test loss = 2.2412338256835938, Test accuracy = 0.5449275374412537\n",
            "Step=6799 loss=4.624606963261613e-06, accuracy=1.0\n",
            "Test loss = 2.252363443374634, Test accuracy = 0.5478261113166809\n",
            "Step=6899 loss=4.310771766995458e-06, accuracy=1.0\n",
            "Test loss = 2.2631497383117676, Test accuracy = 0.5478261113166809\n",
            "Step=6999 loss=4.043586159241386e-06, accuracy=1.0\n",
            "Test loss = 2.2741122245788574, Test accuracy = 0.5507246255874634\n",
            "Step=7099 loss=3.835697468730359e-06, accuracy=1.0\n",
            "Test loss = 2.2848873138427734, Test accuracy = 0.5536231994628906\n",
            "Step=7199 loss=3.5174290201211988e-06, accuracy=1.0\n",
            "Test loss = 2.2957000732421875, Test accuracy = 0.5536231994628906\n",
            "Step=7299 loss=3.3353656249346387e-06, accuracy=1.0\n",
            "Test loss = 2.306150436401367, Test accuracy = 0.5536231994628906\n",
            "Step=7399 loss=3.099676880538027e-06, accuracy=1.0\n",
            "Test loss = 2.3161978721618652, Test accuracy = 0.5536231994628906\n",
            "Step=7499 loss=2.9350382874326898e-06, accuracy=1.0\n",
            "Test loss = 2.327542304992676, Test accuracy = 0.5623188614845276\n",
            "Step=7599 loss=2.7160386389368795e-06, accuracy=1.0\n",
            "Test loss = 2.338315486907959, Test accuracy = 0.5623188614845276\n",
            "Step=7699 loss=2.499516340321861e-06, accuracy=1.0\n",
            "Test loss = 2.3491530418395996, Test accuracy = 0.5623188614845276\n",
            "Step=7799 loss=2.406170146969089e-06, accuracy=1.0\n",
            "Test loss = 2.3604023456573486, Test accuracy = 0.5594202876091003\n",
            "Step=7899 loss=2.2407957249015454e-06, accuracy=1.0\n",
            "Test loss = 2.371325969696045, Test accuracy = 0.5594202876091003\n",
            "Step=7999 loss=2.0954912679371774e-06, accuracy=1.0\n",
            "Test loss = 2.381366491317749, Test accuracy = 0.5594202876091003\n",
            "Step=8099 loss=1.9965384819897737e-06, accuracy=1.0\n",
            "Test loss = 2.3917970657348633, Test accuracy = 0.5594202876091003\n",
            "Step=8199 loss=1.8426006238314585e-06, accuracy=1.0\n",
            "Test loss = 2.401719570159912, Test accuracy = 0.5594202876091003\n",
            "Step=8299 loss=1.7456595196563284e-06, accuracy=1.0\n",
            "Test loss = 2.4127542972564697, Test accuracy = 0.5594202876091003\n",
            "Step=8399 loss=1.6357171466552245e-06, accuracy=1.0\n",
            "Test loss = 2.4227986335754395, Test accuracy = 0.5565217137336731\n",
            "Step=8499 loss=1.5299657093237328e-06, accuracy=1.0\n",
            "Test loss = 2.4327199459075928, Test accuracy = 0.5565217137336731\n",
            "Step=8599 loss=1.4439768790452944e-06, accuracy=1.0\n",
            "Test loss = 2.443068027496338, Test accuracy = 0.5565217137336731\n",
            "Step=8699 loss=1.3669939221472304e-06, accuracy=1.0\n",
            "Test loss = 2.4539191722869873, Test accuracy = 0.5565217137336731\n",
            "Step=8799 loss=1.2717849824639415e-06, accuracy=1.0\n",
            "Test loss = 2.464369773864746, Test accuracy = 0.5565217137336731\n",
            "Step=8899 loss=1.2006506881334645e-06, accuracy=1.0\n",
            "Test loss = 2.4749667644500732, Test accuracy = 0.5565217137336731\n",
            "Step=8999 loss=1.1340519426994434e-06, accuracy=1.0\n",
            "Test loss = 2.485518217086792, Test accuracy = 0.5565217137336731\n",
            "Step=9099 loss=1.05744145571407e-06, accuracy=1.0\n",
            "Test loss = 2.4966914653778076, Test accuracy = 0.5565217137336731\n",
            "Step=9199 loss=9.886726957120118e-07, accuracy=1.0\n",
            "Test loss = 2.5076940059661865, Test accuracy = 0.5536231994628906\n",
            "Step=9299 loss=9.403557561427078e-07, accuracy=1.0\n",
            "Test loss = 2.5180747509002686, Test accuracy = 0.5536231994628906\n",
            "Step=9399 loss=8.802762187087865e-07, accuracy=1.0\n",
            "Test loss = 2.5287840366363525, Test accuracy = 0.5565217137336731\n",
            "Step=9499 loss=8.244900675435929e-07, accuracy=1.0\n",
            "Test loss = 2.5393829345703125, Test accuracy = 0.5565217137336731\n",
            "Step=9599 loss=7.805782627201552e-07, accuracy=1.0\n",
            "Test loss = 2.5507736206054688, Test accuracy = 0.5565217137336731\n",
            "Step=9699 loss=7.295325355016757e-07, accuracy=1.0\n",
            "Test loss = 2.56147837638855, Test accuracy = 0.5594202876091003\n",
            "Step=9799 loss=6.963867969034254e-07, accuracy=1.0\n",
            "Test loss = 2.571300506591797, Test accuracy = 0.5594202876091003\n",
            "Step=9899 loss=6.585658235280789e-07, accuracy=1.0\n",
            "Test loss = 2.581129312515259, Test accuracy = 0.5594202876091003\n",
            "Step=9999 loss=6.126423528485248e-07, accuracy=1.0\n",
            "Test loss = 2.5915255546569824, Test accuracy = 0.5594202876091003\n",
            "Step=10099 loss=5.726327603383652e-07, accuracy=1.0\n",
            "Test loss = 2.6014785766601562, Test accuracy = 0.5594202876091003\n",
            "Step=10199 loss=5.536431132213693e-07, accuracy=1.0\n",
            "Test loss = 2.611635446548462, Test accuracy = 0.5594202876091003\n",
            "Step=10299 loss=5.166882579032972e-07, accuracy=1.0\n",
            "Test loss = 2.6223411560058594, Test accuracy = 0.5594202876091003\n",
            "Step=10399 loss=4.83896413356888e-07, accuracy=1.0\n",
            "Test loss = 2.6328649520874023, Test accuracy = 0.5623188614845276\n",
            "Step=10499 loss=4.597844855425137e-07, accuracy=1.0\n",
            "Test loss = 2.6431119441986084, Test accuracy = 0.5623188614845276\n",
            "Step=10599 loss=4.335584642944923e-07, accuracy=1.0\n",
            "Test loss = 2.6530911922454834, Test accuracy = 0.5623188614845276\n",
            "Step=10699 loss=4.0485511590304666e-07, accuracy=1.0\n",
            "Test loss = 2.6637821197509766, Test accuracy = 0.5623188614845276\n",
            "Step=10799 loss=3.866105205929671e-07, accuracy=1.0\n",
            "Test loss = 2.6742711067199707, Test accuracy = 0.5623188614845276\n",
            "Step=10899 loss=3.6835661063605587e-07, accuracy=1.0\n",
            "Test loss = 2.6848578453063965, Test accuracy = 0.5652173757553101\n",
            "Step=10999 loss=3.4725284791647937e-07, accuracy=1.0\n",
            "Test loss = 2.6960339546203613, Test accuracy = 0.5652173757553101\n",
            "Step=11099 loss=3.239884213712685e-07, accuracy=1.0\n",
            "Test loss = 2.7070934772491455, Test accuracy = 0.5652173757553101\n",
            "Step=11199 loss=3.0158081131048676e-07, accuracy=1.0\n",
            "Test loss = 2.717571258544922, Test accuracy = 0.5681159496307373\n",
            "Step=11299 loss=2.815666941557993e-07, accuracy=1.0\n",
            "Test loss = 2.7279462814331055, Test accuracy = 0.5681159496307373\n",
            "Step=11399 loss=2.6044430271099375e-07, accuracy=1.0\n",
            "Test loss = 2.7385618686676025, Test accuracy = 0.5681159496307373\n",
            "Step=11499 loss=2.4795526996967966e-07, accuracy=1.0\n",
            "Test loss = 2.7492294311523438, Test accuracy = 0.5652173757553101\n",
            "Step=11599 loss=2.3297961107004995e-07, accuracy=1.0\n",
            "Test loss = 2.759089231491089, Test accuracy = 0.5652173757553101\n",
            "Step=11699 loss=2.2267918737384206e-07, accuracy=1.0\n",
            "Test loss = 2.7678961753845215, Test accuracy = 0.5652173757553101\n",
            "Step=11799 loss=2.1565701814552084e-07, accuracy=1.0\n",
            "Test loss = 2.7767581939697266, Test accuracy = 0.5652173757553101\n",
            "Step=11899 loss=2.0265575884081954e-07, accuracy=1.0\n",
            "Test loss = 2.7865331172943115, Test accuracy = 0.5681159496307373\n",
            "Step=11999 loss=1.937523175854494e-07, accuracy=1.0\n",
            "Test loss = 2.7954628467559814, Test accuracy = 0.5681159496307373\n",
            "Step=12099 loss=1.7682087474213404e-07, accuracy=1.0\n",
            "Test loss = 2.8052818775177, Test accuracy = 0.5681159496307373\n",
            "Step=12199 loss=1.7108392867726253e-07, accuracy=1.0\n",
            "Test loss = 2.8149096965789795, Test accuracy = 0.5681159496307373\n",
            "Step=12299 loss=1.6237606359936762e-07, accuracy=1.0\n",
            "Test loss = 2.8239073753356934, Test accuracy = 0.5681159496307373\n",
            "Step=12399 loss=1.5518625403387888e-07, accuracy=1.0\n",
            "Test loss = 2.8326611518859863, Test accuracy = 0.5681159496307373\n",
            "Step=12499 loss=1.5477647210104807e-07, accuracy=1.0\n",
            "Test loss = 2.841783285140991, Test accuracy = 0.5681159496307373\n",
            "Step=12599 loss=1.5292314046178034e-07, accuracy=1.0\n",
            "Test loss = 2.850912094116211, Test accuracy = 0.5681159496307373\n",
            "Step=12699 loss=1.4754941020100886e-07, accuracy=1.0\n",
            "Test loss = 2.8604555130004883, Test accuracy = 0.5681159496307373\n",
            "Step=12799 loss=1.3314185615342921e-07, accuracy=1.0\n",
            "Test loss = 2.870612621307373, Test accuracy = 0.5681159496307373\n",
            "Step=12899 loss=1.1136753883533856e-07, accuracy=1.0\n",
            "Test loss = 2.87934947013855, Test accuracy = 0.5681159496307373\n",
            "Step=12999 loss=9.587964633794854e-08, accuracy=1.0\n",
            "Test loss = 2.8907504081726074, Test accuracy = 0.5710144639015198\n",
            "Step=13099 loss=8.872709038598714e-08, accuracy=1.0\n",
            "Test loss = 2.901257276535034, Test accuracy = 0.5681159496307373\n",
            "Step=13199 loss=8.266418184632585e-08, accuracy=1.0\n",
            "Test loss = 2.9107179641723633, Test accuracy = 0.5681159496307373\n",
            "Step=13299 loss=8.033587519662433e-08, accuracy=1.0\n",
            "Test loss = 2.920159339904785, Test accuracy = 0.5681159496307373\n",
            "Step=13399 loss=7.227993712888292e-08, accuracy=1.0\n",
            "Test loss = 2.929511547088623, Test accuracy = 0.5681159496307373\n",
            "Step=13499 loss=7.119960311996465e-08, accuracy=1.0\n",
            "Test loss = 2.93790340423584, Test accuracy = 0.5681159496307373\n",
            "Step=13599 loss=7.203779283315725e-08, accuracy=1.0\n",
            "Test loss = 2.9485273361206055, Test accuracy = 0.5652173757553101\n",
            "Step=13699 loss=6.682238741007041e-08, accuracy=1.0\n",
            "Test loss = 2.957699775695801, Test accuracy = 0.5652173757553101\n",
            "Step=13799 loss=6.265006273764584e-08, accuracy=1.0\n",
            "Test loss = 2.9672796726226807, Test accuracy = 0.5652173757553101\n",
            "Step=13899 loss=5.8747821221061256e-08, accuracy=1.0\n",
            "Test loss = 2.978585958480835, Test accuracy = 0.5652173757553101\n",
            "Step=13999 loss=5.166977075532486e-08, accuracy=1.0\n",
            "Test loss = 2.987959146499634, Test accuracy = 0.5652173757553101\n",
            "Step=14099 loss=5.008652259164137e-08, accuracy=1.0\n",
            "Test loss = 2.9959733486175537, Test accuracy = 0.5652173757553101\n",
            "Step=14199 loss=4.666856913360107e-08, accuracy=1.0\n",
            "Test loss = 3.0014214515686035, Test accuracy = 0.5652173757553101\n",
            "Step=14299 loss=4.7301868484339593e-08, accuracy=1.0\n",
            "Test loss = 3.0075523853302, Test accuracy = 0.5710144639015198\n",
            "Step=14399 loss=4.2160968600768453e-08, accuracy=1.0\n",
            "Test loss = 3.0169484615325928, Test accuracy = 0.5710144639015198\n",
            "Step=14499 loss=3.8258727421691674e-08, accuracy=1.0\n",
            "Test loss = 3.0264904499053955, Test accuracy = 0.5710144639015198\n",
            "Step=14599 loss=3.919004996078002e-08, accuracy=1.0\n",
            "Test loss = 3.0368428230285645, Test accuracy = 0.5710144639015198\n",
            "Step=14699 loss=3.223307093236372e-08, accuracy=1.0\n",
            "Test loss = 3.0478556156158447, Test accuracy = 0.5681159496307373\n",
            "Step=14799 loss=2.0917503098516476e-08, accuracy=1.0\n",
            "Test loss = 3.056563138961792, Test accuracy = 0.5681159496307373\n",
            "Step=14899 loss=1.878477459360539e-08, accuracy=1.0\n",
            "Test loss = 3.063673496246338, Test accuracy = 0.5681159496307373\n",
            "Step=14999 loss=1.1511146025267039e-08, accuracy=1.0\n",
            "Test loss = 3.068375825881958, Test accuracy = 0.5681159496307373\n",
            "Step=15099 loss=9.099020772951327e-09, accuracy=1.0\n",
            "Test loss = 3.074862241744995, Test accuracy = 0.5681159496307373\n",
            "Step=15199 loss=7.888301521430385e-09, accuracy=1.0\n",
            "Test loss = 3.0822184085845947, Test accuracy = 0.5681159496307373\n",
            "Step=15299 loss=8.149071814567144e-09, accuracy=1.0\n",
            "Test loss = 3.089545965194702, Test accuracy = 0.5710144639015198\n",
            "Step=15399 loss=7.2550022356399065e-09, accuracy=1.0\n",
            "Test loss = 3.0983002185821533, Test accuracy = 0.5681159496307373\n",
            "Step=15499 loss=6.565823612025845e-09, accuracy=1.0\n",
            "Test loss = 3.106907844543457, Test accuracy = 0.5681159496307373\n",
            "Step=15599 loss=6.752088106409815e-09, accuracy=1.0\n",
            "Test loss = 3.1165316104888916, Test accuracy = 0.5681159496307373\n",
            "Step=15699 loss=6.034969799451062e-09, accuracy=1.0\n",
            "Test loss = 3.1272668838500977, Test accuracy = 0.5681159496307373\n",
            "Step=15799 loss=5.9511507294329745e-09, accuracy=1.0\n",
            "Test loss = 3.138381242752075, Test accuracy = 0.5681159496307373\n",
            "Step=15899 loss=5.643814358635701e-09, accuracy=1.0\n",
            "Test loss = 3.1505072116851807, Test accuracy = 0.5710144639015198\n",
            "Step=15999 loss=5.103647284787627e-09, accuracy=1.0\n",
            "Test loss = 3.163470506668091, Test accuracy = 0.5710144639015198\n",
            "Step=16099 loss=4.9639489185793195e-09, accuracy=1.0\n",
            "Test loss = 3.177419662475586, Test accuracy = 0.5710144639015198\n",
            "Step=16199 loss=4.461034776026551e-09, accuracy=1.0\n",
            "Test loss = 3.191175699234009, Test accuracy = 0.5710144639015198\n",
            "Step=16299 loss=4.898756355564693e-09, accuracy=1.0\n",
            "Test loss = 3.2052578926086426, Test accuracy = 0.5710144639015198\n",
            "Step=16399 loss=5.364417585140835e-09, accuracy=1.0\n",
            "Test loss = 3.2200918197631836, Test accuracy = 0.5652173757553101\n",
            "Step=16499 loss=4.6938653947004025e-09, accuracy=1.0\n",
            "Test loss = 3.235973834991455, Test accuracy = 0.5652173757553101\n",
            "Step=16599 loss=4.1164454556152916e-09, accuracy=1.0\n",
            "Test loss = 3.252232789993286, Test accuracy = 0.5652173757553101\n",
            "Step=16699 loss=3.6787238855140457e-09, accuracy=1.0\n",
            "Test loss = 3.268878221511841, Test accuracy = 0.5623188614845276\n",
            "Step=16799 loss=2.4028120509944627e-09, accuracy=1.0\n",
            "Test loss = 3.2869138717651367, Test accuracy = 0.5594202876091003\n",
            "Step=16899 loss=1.993030160907239e-09, accuracy=1.0\n",
            "Test loss = 3.3071045875549316, Test accuracy = 0.5594202876091003\n",
            "Step=16999 loss=2.3376194796531634e-09, accuracy=1.0\n",
            "Test loss = 3.327524185180664, Test accuracy = 0.5594202876091003\n",
            "Step=17099 loss=1.965090478117659e-09, accuracy=1.0\n",
            "Test loss = 3.3506128787994385, Test accuracy = 0.5594202876091003\n",
            "Step=17199 loss=2.3003665733378754e-09, accuracy=1.0\n",
            "Test loss = 3.3730196952819824, Test accuracy = 0.5536231994628906\n",
            "Step=17299 loss=2.22586078679754e-09, accuracy=1.0\n",
            "Test loss = 3.397352695465088, Test accuracy = 0.5536231994628906\n",
            "Step=17399 loss=1.9092111341878493e-09, accuracy=1.0\n",
            "Test loss = 3.4230895042419434, Test accuracy = 0.5536231994628906\n",
            "Step=17499 loss=1.8905846799199823e-09, accuracy=1.0\n",
            "Test loss = 3.4492480754852295, Test accuracy = 0.5507246255874634\n",
            "Step=17599 loss=2.0209698264883612e-09, accuracy=1.0\n",
            "Test loss = 3.476609230041504, Test accuracy = 0.5507246255874634\n",
            "Step=17699 loss=1.8253921074684598e-09, accuracy=1.0\n",
            "Test loss = 3.504538059234619, Test accuracy = 0.5507246255874634\n",
            "Step=17799 loss=1.927837580684155e-09, accuracy=1.0\n",
            "Test loss = 3.533822536468506, Test accuracy = 0.5507246255874634\n",
            "Step=17899 loss=2.1327285326666613e-09, accuracy=1.0\n",
            "Test loss = 3.563786268234253, Test accuracy = 0.5478261113166809\n",
            "Step=17999 loss=2.2258607818015363e-09, accuracy=1.0\n",
            "Test loss = 3.5966901779174805, Test accuracy = 0.5507246255874634\n",
            "Step=18099 loss=2.5890765531499936e-09, accuracy=1.0\n",
            "Test loss = 3.6319050788879395, Test accuracy = 0.5478261113166809\n",
            "Step=18199 loss=2.4586914054713915e-09, accuracy=1.0\n",
            "Test loss = 3.6730544567108154, Test accuracy = 0.5478261113166809\n",
            "Step=18299 loss=2.0023433833227242e-09, accuracy=1.0\n",
            "Test loss = 3.716726779937744, Test accuracy = 0.5507246255874634\n",
            "Step=18399 loss=1.983716928499746e-09, accuracy=1.0\n",
            "Test loss = 3.7610831260681152, Test accuracy = 0.5507246255874634\n",
            "Step=18499 loss=1.7788259804030204e-09, accuracy=1.0\n",
            "Test loss = 3.8061413764953613, Test accuracy = 0.5478261113166809\n",
            "Step=18599 loss=1.7974524207930998e-09, accuracy=1.0\n",
            "Test loss = 3.8537778854370117, Test accuracy = 0.5478261113166809\n",
            "Step=18699 loss=1.8347053193368268e-09, accuracy=1.0\n",
            "Test loss = 3.9029316902160645, Test accuracy = 0.5478261113166809\n",
            "Step=18799 loss=1.844018558405658e-09, accuracy=1.0\n",
            "Test loss = 3.9752190113067627, Test accuracy = 0.5536231994628906\n",
            "Step=18899 loss=2.4214385008214377e-09, accuracy=1.0\n",
            "Test loss = 4.055225849151611, Test accuracy = 0.5565217137336731\n",
            "Step=18999 loss=3.930180937916638e-09, accuracy=1.0\n",
            "Test loss = 4.13911771774292, Test accuracy = 0.5594202876091003\n",
            "Step=19099 loss=3.9767470733087505e-09, accuracy=1.0\n",
            "Test loss = 4.225459575653076, Test accuracy = 0.5536231994628906\n",
            "Step=19199 loss=3.93949415866679e-09, accuracy=1.0\n",
            "Test loss = 4.322824001312256, Test accuracy = 0.5565217137336731\n",
            "Step=19299 loss=5.802139159682973e-09, accuracy=1.0\n",
            "Test loss = 4.446571350097656, Test accuracy = 0.5565217137336731\n",
            "Step=19399 loss=5.6903804768193566e-09, accuracy=1.0\n",
            "Test loss = 4.56699275970459, Test accuracy = 0.5594202876091003\n",
            "Step=19499 loss=5.820765620057067e-09, accuracy=1.0\n",
            "Test loss = 4.7180609703063965, Test accuracy = 0.5652173757553101\n",
            "Step=19599 loss=6.5937632615087334e-09, accuracy=1.0\n",
            "Test loss = 4.912133693695068, Test accuracy = 0.5536231994628906\n",
            "Step=19699 loss=6.2678004331129245e-09, accuracy=1.0\n",
            "Test loss = 5.1543731689453125, Test accuracy = 0.5536231994628906\n",
            "Step=19799 loss=8.111818947664773e-09, accuracy=1.0\n",
            "Test loss = 5.387126922607422, Test accuracy = 0.5565217137336731\n",
            "Step=19899 loss=8.307396634488207e-09, accuracy=1.0\n",
            "Test loss = 5.673933506011963, Test accuracy = 0.5623188614845276\n",
            "Step=19999 loss=8.549540497782004e-09, accuracy=1.0\n",
            "Test loss = 5.941738605499268, Test accuracy = 0.5594202876091003\n",
            "Step=20099 loss=9.918584517598639e-09, accuracy=1.0\n",
            "Test loss = 6.1835246086120605, Test accuracy = 0.5565217137336731\n",
            "Step=20199 loss=9.071081072953292e-09, accuracy=1.0\n",
            "Test loss = 6.609547138214111, Test accuracy = 0.5565217137336731\n",
            "Step=20299 loss=7.674097362575516e-09, accuracy=1.0\n",
            "Test loss = 7.1641154289245605, Test accuracy = 0.5478261113166809\n",
            "Step=20399 loss=6.472691347902959e-09, accuracy=1.0\n",
            "Test loss = 7.914926052093506, Test accuracy = 0.5536231994628906\n",
            "Step=20499 loss=5.820765603958833e-09, accuracy=1.0\n",
            "Test loss = 8.81155014038086, Test accuracy = 0.5478261113166809\n",
            "Step=20599 loss=4.9918885813848845e-09, accuracy=1.0\n",
            "Test loss = 10.142131805419922, Test accuracy = 0.5420289635658264\n",
            "Step=20699 loss=5.299225009913755e-09, accuracy=1.0\n",
            "Test loss = 11.915889739990234, Test accuracy = 0.5246376991271973\n",
            "Step=20799 loss=5.299225014909758e-09, accuracy=1.0\n",
            "Test loss = 14.224095344543457, Test accuracy = 0.5246376991271973\n",
            "Step=20899 loss=4.936009239120409e-09, accuracy=1.0\n",
            "Test loss = 16.91022300720215, Test accuracy = 0.5275362133979797\n",
            "Step=20999 loss=3.9581206123795455e-09, accuracy=1.0\n",
            "Test loss = 20.24738121032715, Test accuracy = 0.5275362133979797\n",
            "Step=21099 loss=6.128102025271253e-09, accuracy=1.0\n",
            "Test loss = 24.381723403930664, Test accuracy = 0.5188405513763428\n",
            "Step=21199 loss=0.06423246190997771, accuracy=0.9903125\n",
            "Test loss = 24.01869010925293, Test accuracy = 0.5246376991271973\n",
            "Step=21299 loss=3.705073498991851e-05, accuracy=1.0\n",
            "Test loss = 24.245582580566406, Test accuracy = 0.5188405513763428\n",
            "Step=21399 loss=8.644351430575625e-06, accuracy=1.0\n",
            "Test loss = 24.285593032836914, Test accuracy = 0.5188405513763428\n",
            "Step=21499 loss=5.270488391033723e-06, accuracy=1.0\n",
            "Test loss = 24.30933952331543, Test accuracy = 0.52173912525177\n",
            "Step=21599 loss=3.700974551748004e-06, accuracy=1.0\n",
            "Test loss = 24.32525634765625, Test accuracy = 0.52173912525177\n",
            "Step=21699 loss=3.06816185769776e-06, accuracy=1.0\n",
            "Test loss = 24.338388442993164, Test accuracy = 0.5246376991271973\n",
            "Step=21799 loss=2.6396765855452033e-06, accuracy=1.0\n",
            "Test loss = 24.349748611450195, Test accuracy = 0.5246376991271973\n",
            "Step=21899 loss=2.3958843814853027e-06, accuracy=1.0\n",
            "Test loss = 24.360612869262695, Test accuracy = 0.5246376991271973\n",
            "Step=21999 loss=2.1182175839840056e-06, accuracy=1.0\n",
            "Test loss = 24.370590209960938, Test accuracy = 0.5246376991271973\n",
            "Step=22099 loss=1.6859410348502023e-06, accuracy=1.0\n",
            "Test loss = 24.378807067871094, Test accuracy = 0.5246376991271973\n",
            "Step=22199 loss=1.5695870803256184e-06, accuracy=1.0\n",
            "Test loss = 24.386829376220703, Test accuracy = 0.5246376991271973\n",
            "Step=22299 loss=1.496826581046662e-06, accuracy=1.0\n",
            "Test loss = 24.394798278808594, Test accuracy = 0.5246376991271973\n",
            "Step=22399 loss=1.2982740369693601e-06, accuracy=1.0\n",
            "Test loss = 24.401948928833008, Test accuracy = 0.5246376991271973\n",
            "Step=22499 loss=1.1835010738536766e-06, accuracy=1.0\n",
            "Test loss = 24.408803939819336, Test accuracy = 0.5246376991271973\n",
            "Step=22599 loss=1.0897284884947567e-06, accuracy=1.0\n",
            "Test loss = 24.415369033813477, Test accuracy = 0.5246376991271973\n",
            "Step=22699 loss=1.0218276584339492e-06, accuracy=1.0\n",
            "Test loss = 24.421859741210938, Test accuracy = 0.5246376991271973\n",
            "Step=22799 loss=9.164502129976882e-07, accuracy=1.0\n",
            "Test loss = 24.42770767211914, Test accuracy = 0.5246376991271973\n",
            "Step=22899 loss=8.907467045560224e-07, accuracy=1.0\n",
            "Test loss = 24.433687210083008, Test accuracy = 0.52173912525177\n",
            "Step=22999 loss=8.151337074480125e-07, accuracy=1.0\n",
            "Test loss = 24.4395751953125, Test accuracy = 0.52173912525177\n",
            "Step=23099 loss=7.421098743520815e-07, accuracy=1.0\n",
            "Test loss = 24.444950103759766, Test accuracy = 0.52173912525177\n",
            "Step=23199 loss=6.975746501325375e-07, accuracy=1.0\n",
            "Test loss = 24.450246810913086, Test accuracy = 0.52173912525177\n",
            "Step=23299 loss=6.374025620914381e-07, accuracy=1.0\n",
            "Test loss = 24.455318450927734, Test accuracy = 0.52173912525177\n",
            "Step=23399 loss=5.926810300849183e-07, accuracy=1.0\n",
            "Test loss = 24.460227966308594, Test accuracy = 0.52173912525177\n",
            "Step=23499 loss=5.716707116221187e-07, accuracy=1.0\n",
            "Test loss = 24.465177536010742, Test accuracy = 0.52173912525177\n",
            "Step=23599 loss=5.262225914748342e-07, accuracy=1.0\n",
            "Test loss = 24.469926834106445, Test accuracy = 0.5188405513763428\n",
            "Step=23699 loss=5.10362365275796e-07, accuracy=1.0\n",
            "Test loss = 24.474889755249023, Test accuracy = 0.5159420371055603\n",
            "Step=23799 loss=4.5604806828691837e-07, accuracy=1.0\n",
            "Test loss = 24.479421615600586, Test accuracy = 0.5159420371055603\n",
            "Step=23899 loss=4.592612219767034e-07, accuracy=1.0\n",
            "Test loss = 24.484399795532227, Test accuracy = 0.5159420371055603\n",
            "Step=23999 loss=4.580319814095901e-07, accuracy=1.0\n",
            "Test loss = 24.48966407775879, Test accuracy = 0.5159420371055603\n",
            "Step=24099 loss=4.1564775585811733e-07, accuracy=1.0\n",
            "Test loss = 24.494508743286133, Test accuracy = 0.5159420371055603\n",
            "Step=24199 loss=3.837594755395912e-07, accuracy=1.0\n",
            "Test loss = 24.499366760253906, Test accuracy = 0.5159420371055603\n",
            "Step=24299 loss=3.5222508799392926e-07, accuracy=1.0\n",
            "Test loss = 24.503896713256836, Test accuracy = 0.5159420371055603\n",
            "Step=24399 loss=3.3530306300377786e-07, accuracy=1.0\n",
            "Test loss = 24.50856590270996, Test accuracy = 0.5159420371055603\n",
            "Step=24499 loss=3.220318108532183e-07, accuracy=1.0\n",
            "Test loss = 24.513141632080078, Test accuracy = 0.5159420371055603\n",
            "Step=24599 loss=3.0502596153780815e-07, accuracy=1.0\n",
            "Test loss = 24.517778396606445, Test accuracy = 0.5159420371055603\n",
            "Step=24699 loss=2.86483415052885e-07, accuracy=1.0\n",
            "Test loss = 24.522436141967773, Test accuracy = 0.5159420371055603\n",
            "Step=24799 loss=2.9505160078713286e-07, accuracy=1.0\n",
            "Test loss = 24.527441024780273, Test accuracy = 0.5159420371055603\n",
            "Step=24899 loss=2.6684194651238616e-07, accuracy=1.0\n",
            "Test loss = 24.532224655151367, Test accuracy = 0.5159420371055603\n",
            "Step=24999 loss=2.6201773941636473e-07, accuracy=1.0\n",
            "Test loss = 24.537111282348633, Test accuracy = 0.5130434632301331\n",
            "Step=25099 loss=2.3498155258039332e-07, accuracy=1.0\n",
            "Test loss = 24.541854858398438, Test accuracy = 0.5130434632301331\n",
            "Step=25199 loss=2.361829577424146e-07, accuracy=1.0\n",
            "Test loss = 24.546783447265625, Test accuracy = 0.5130434632301331\n",
            "Step=25299 loss=2.1250882006995653e-07, accuracy=1.0\n",
            "Test loss = 24.551511764526367, Test accuracy = 0.5130434632301331\n",
            "Step=25399 loss=2.170164342274461e-07, accuracy=1.0\n",
            "Test loss = 24.556684494018555, Test accuracy = 0.5130434632301331\n",
            "Step=25499 loss=2.0625038111177218e-07, accuracy=1.0\n",
            "Test loss = 24.561756134033203, Test accuracy = 0.5130434632301331\n",
            "Step=25599 loss=1.8333058044106565e-07, accuracy=1.0\n",
            "Test loss = 24.566499710083008, Test accuracy = 0.5130434632301331\n",
            "Step=25699 loss=1.6421056962201418e-07, accuracy=1.0\n",
            "Test loss = 24.570932388305664, Test accuracy = 0.5130434632301331\n",
            "Step=25799 loss=1.5707666733533188e-07, accuracy=1.0\n",
            "Test loss = 24.575416564941406, Test accuracy = 0.5130434632301331\n",
            "Step=25899 loss=1.6499289927196513e-07, accuracy=1.0\n",
            "Test loss = 24.580381393432617, Test accuracy = 0.5130434632301331\n",
            "Step=25999 loss=1.5242938488313485e-07, accuracy=1.0\n",
            "Test loss = 24.585193634033203, Test accuracy = 0.5130434632301331\n",
            "Step=26099 loss=1.41272160334438e-07, accuracy=1.0\n",
            "Test loss = 24.589937210083008, Test accuracy = 0.5130434632301331\n",
            "Step=26199 loss=1.242848678728592e-07, accuracy=1.0\n",
            "Test loss = 24.594318389892578, Test accuracy = 0.5130434632301331\n",
            "Step=26299 loss=1.2591468319556044e-07, accuracy=1.0\n",
            "Test loss = 24.598955154418945, Test accuracy = 0.5130434632301331\n",
            "Step=26399 loss=1.2567255257778243e-07, accuracy=1.0\n",
            "Test loss = 24.60382652282715, Test accuracy = 0.5130434632301331\n",
            "Step=26499 loss=1.1629414498059987e-07, accuracy=1.0\n",
            "Test loss = 24.608612060546875, Test accuracy = 0.5130434632301331\n",
            "Step=26599 loss=1.1859451433338109e-07, accuracy=1.0\n",
            "Test loss = 24.613801956176758, Test accuracy = 0.5130434632301331\n",
            "Step=26699 loss=1.0337672424753919e-07, accuracy=1.0\n",
            "Test loss = 24.61860466003418, Test accuracy = 0.5130434632301331\n",
            "Step=26799 loss=9.732313571930717e-08, accuracy=1.0\n",
            "Test loss = 24.623340606689453, Test accuracy = 0.5130434632301331\n",
            "Step=26899 loss=9.613104491990043e-08, accuracy=1.0\n",
            "Test loss = 24.628170013427734, Test accuracy = 0.5130434632301331\n",
            "Step=26999 loss=9.309493732345687e-08, accuracy=1.0\n",
            "Test loss = 24.633203506469727, Test accuracy = 0.5130434632301331\n",
            "Step=27099 loss=8.533702665403098e-08, accuracy=1.0\n",
            "Test loss = 24.637836456298828, Test accuracy = 0.5130434632301331\n",
            "Step=27199 loss=8.068042244957497e-08, accuracy=1.0\n",
            "Test loss = 24.64263343811035, Test accuracy = 0.5130434632301331\n",
            "Step=27299 loss=7.227058855363566e-08, accuracy=1.0\n",
            "Test loss = 24.6472110748291, Test accuracy = 0.5130434632301331\n",
            "Step=27399 loss=7.35092470982579e-08, accuracy=1.0\n",
            "Test loss = 24.65190887451172, Test accuracy = 0.5130434632301331\n",
            "Step=27499 loss=6.671991224393992e-08, accuracy=1.0\n",
            "Test loss = 24.65662956237793, Test accuracy = 0.5130434632301331\n",
            "Step=27599 loss=6.081533125978922e-08, accuracy=1.0\n",
            "Test loss = 24.6608943939209, Test accuracy = 0.5130434632301331\n",
            "Step=27699 loss=5.988401204248816e-08, accuracy=1.0\n",
            "Test loss = 24.665441513061523, Test accuracy = 0.5130434632301331\n",
            "Step=27799 loss=5.88781858823495e-08, accuracy=1.0\n",
            "Test loss = 24.670230865478516, Test accuracy = 0.5130434632301331\n",
            "Step=27899 loss=5.5730318493374396e-08, accuracy=1.0\n",
            "Test loss = 24.675020217895508, Test accuracy = 0.5130434632301331\n",
            "Step=27999 loss=5.1772199984867485e-08, accuracy=1.0\n",
            "Test loss = 24.679445266723633, Test accuracy = 0.5130434632301331\n",
            "Step=28099 loss=4.686413414667357e-08, accuracy=1.0\n",
            "Test loss = 24.684009552001953, Test accuracy = 0.5130434632301331\n",
            "Step=28199 loss=4.9434583289098554e-08, accuracy=1.0\n",
            "Test loss = 24.688716888427734, Test accuracy = 0.5130434632301331\n",
            "Step=28299 loss=4.3762832167004714e-08, accuracy=1.0\n",
            "Test loss = 24.693462371826172, Test accuracy = 0.5130434632301331\n",
            "Step=28399 loss=4.193744092439999e-08, accuracy=1.0\n",
            "Test loss = 24.698205947875977, Test accuracy = 0.5101449489593506\n",
            "Step=28499 loss=4.250554722418087e-08, accuracy=1.0\n",
            "Test loss = 24.70291519165039, Test accuracy = 0.5101449489593506\n",
            "Step=28599 loss=3.655440193206516e-08, accuracy=1.0\n",
            "Test loss = 24.707548141479492, Test accuracy = 0.5101449489593506\n",
            "Step=28699 loss=3.579071870873918e-08, accuracy=1.0\n",
            "Test loss = 24.71221923828125, Test accuracy = 0.5101449489593506\n",
            "Step=28799 loss=3.587453737630142e-08, accuracy=1.0\n",
            "Test loss = 24.71676254272461, Test accuracy = 0.5101449489593506\n",
            "Step=28899 loss=3.215856135163619e-08, accuracy=1.0\n",
            "Test loss = 24.721416473388672, Test accuracy = 0.5101449489593506\n",
            "Step=28999 loss=2.9737123199424785e-08, accuracy=1.0\n",
            "Test loss = 24.726198196411133, Test accuracy = 0.5101449489593506\n",
            "Step=29099 loss=3.163702069208796e-08, accuracy=1.0\n",
            "Test loss = 24.73133087158203, Test accuracy = 0.5072463750839233\n",
            "Step=29199 loss=3.109685373203774e-08, accuracy=1.0\n",
            "Test loss = 24.736244201660156, Test accuracy = 0.5072463750839233\n",
            "Step=29299 loss=2.625397789834949e-08, accuracy=1.0\n",
            "Test loss = 24.740863800048828, Test accuracy = 0.5072463750839233\n",
            "Step=29399 loss=2.3823227155439496e-08, accuracy=1.0\n",
            "Test loss = 24.74580192565918, Test accuracy = 0.5072463750839233\n",
            "Step=29499 loss=2.4754549796668356e-08, accuracy=1.0\n",
            "Test loss = 24.750829696655273, Test accuracy = 0.5072463750839233\n",
            "Step=29599 loss=2.2780146213818853e-08, accuracy=1.0\n",
            "Test loss = 24.755334854125977, Test accuracy = 0.5072463750839233\n",
            "Step=29699 loss=2.4177129982483336e-08, accuracy=1.0\n",
            "Test loss = 24.76006317138672, Test accuracy = 0.5101449489593506\n",
            "Step=29799 loss=2.2612508248087692e-08, accuracy=1.0\n",
            "Test loss = 24.76534652709961, Test accuracy = 0.5101449489593506\n",
            "Step=29899 loss=2.041458736279367e-08, accuracy=1.0\n",
            "Test loss = 24.771352767944336, Test accuracy = 0.5101449489593506\n",
            "Step=29999 loss=2.0815056198664195e-08, accuracy=1.0\n",
            "Test loss = 24.777267456054688, Test accuracy = 0.5101449489593506\n",
            "Step=30099 loss=1.8915158386256793e-08, accuracy=1.0\n",
            "Test loss = 24.782562255859375, Test accuracy = 0.5101449489593506\n",
            "Step=30199 loss=2.008862463043215e-08, accuracy=1.0\n",
            "Test loss = 24.78802490234375, Test accuracy = 0.5101449489593506\n",
            "Step=30299 loss=1.6149131105702707e-08, accuracy=1.0\n",
            "Test loss = 24.79210662841797, Test accuracy = 0.5101449489593506\n",
            "Step=30399 loss=1.5897674097153924e-08, accuracy=1.0\n",
            "Test loss = 24.795799255371094, Test accuracy = 0.5101449489593506\n",
            "Step=30499 loss=1.1837108655488038e-08, accuracy=1.0\n",
            "Test loss = 24.8010196685791, Test accuracy = 0.5101449489593506\n",
            "Step=30599 loss=1.1576338360130833e-08, accuracy=1.0\n",
            "Test loss = 24.806175231933594, Test accuracy = 0.5101449489593506\n",
            "Step=30699 loss=1.179054251787548e-08, accuracy=1.0\n",
            "Test loss = 24.81131362915039, Test accuracy = 0.5101449489593506\n",
            "Step=30799 loss=1.171603672966981e-08, accuracy=1.0\n",
            "Test loss = 24.816415786743164, Test accuracy = 0.5101449489593506\n",
            "Step=30899 loss=1.1324881277197108e-08, accuracy=1.0\n",
            "Test loss = 24.821176528930664, Test accuracy = 0.5101449489593506\n",
            "Step=30999 loss=1.12038094246536e-08, accuracy=1.0\n",
            "Test loss = 24.8258113861084, Test accuracy = 0.5101449489593506\n",
            "Step=31099 loss=1.1483206105999955e-08, accuracy=1.0\n",
            "Test loss = 24.830320358276367, Test accuracy = 0.5101449489593506\n",
            "Step=31199 loss=1.1110677140546699e-08, accuracy=1.0\n",
            "Test loss = 24.834749221801758, Test accuracy = 0.5101449489593506\n",
            "Step=31299 loss=1.2172384773467826e-08, accuracy=1.0\n",
            "Test loss = 24.839509963989258, Test accuracy = 0.5101449489593506\n",
            "Step=31399 loss=1.175328965874467e-08, accuracy=1.0\n",
            "Test loss = 24.843870162963867, Test accuracy = 0.5101449489593506\n",
            "Step=31499 loss=8.39121564011336e-09, accuracy=1.0\n",
            "Test loss = 24.850799560546875, Test accuracy = 0.5101449489593506\n",
            "Step=31599 loss=7.785856079300935e-09, accuracy=1.0\n",
            "Test loss = 24.857988357543945, Test accuracy = 0.5101449489593506\n",
            "Step=31699 loss=7.757916369310892e-09, accuracy=1.0\n",
            "Test loss = 24.865171432495117, Test accuracy = 0.5101449489593506\n",
            "Step=31799 loss=7.97212056147245e-09, accuracy=1.0\n",
            "Test loss = 24.8726806640625, Test accuracy = 0.5101449489593506\n",
            "Step=31899 loss=8.04662636078035e-09, accuracy=1.0\n",
            "Test loss = 24.880184173583984, Test accuracy = 0.5101449489593506\n",
            "Step=31999 loss=7.543712196023123e-09, accuracy=1.0\n",
            "Test loss = 24.887399673461914, Test accuracy = 0.5101449489593506\n",
            "Step=32099 loss=7.888301540859289e-09, accuracy=1.0\n",
            "Test loss = 24.894752502441406, Test accuracy = 0.5101449489593506\n",
            "Step=32199 loss=8.046626391866595e-09, accuracy=1.0\n",
            "Test loss = 24.902299880981445, Test accuracy = 0.5072463750839233\n",
            "Step=32299 loss=8.512287619222292e-09, accuracy=1.0\n",
            "Test loss = 24.91037368774414, Test accuracy = 0.5072463750839233\n",
            "Step=32399 loss=6.0163433390769685e-09, accuracy=1.0\n",
            "Test loss = 24.915998458862305, Test accuracy = 0.5072463750839233\n",
            "Step=32499 loss=4.0139999873956e-09, accuracy=1.0\n",
            "Test loss = 24.918453216552734, Test accuracy = 0.5101449489593506\n",
            "Step=32599 loss=3.948807422160528e-09, accuracy=1.0\n",
            "Test loss = 24.920698165893555, Test accuracy = 0.5101449489593506\n",
            "Step=32699 loss=3.967433870322168e-09, accuracy=1.0\n",
            "Test loss = 24.922710418701172, Test accuracy = 0.5101449489593506\n",
            "Step=32799 loss=4.153698360820357e-09, accuracy=1.0\n",
            "Test loss = 24.924495697021484, Test accuracy = 0.5101449489593506\n",
            "Step=32899 loss=3.5855916413751742e-09, accuracy=1.0\n",
            "Test loss = 24.925615310668945, Test accuracy = 0.5101449489593506\n",
            "Step=32999 loss=4.339962857424773e-09, accuracy=1.0\n",
            "Test loss = 24.92684555053711, Test accuracy = 0.5101449489593506\n",
            "Step=33099 loss=4.135071927646727e-09, accuracy=1.0\n",
            "Test loss = 24.92752456665039, Test accuracy = 0.5101449489593506\n",
            "Step=33199 loss=3.97674708774165e-09, accuracy=1.0\n",
            "Test loss = 24.927743911743164, Test accuracy = 0.5101449489593506\n",
            "Step=33299 loss=4.1816380436099366e-09, accuracy=1.0\n",
            "Test loss = 24.92741584777832, Test accuracy = 0.5101449489593506\n",
            "Step=33399 loss=4.032626449435029e-09, accuracy=1.0\n",
            "Test loss = 24.92668342590332, Test accuracy = 0.5101449489593506\n",
            "Step=33499 loss=4.00468671557519e-10, accuracy=1.0\n",
            "Test loss = 24.93325424194336, Test accuracy = 0.5101449489593506\n",
            "Step=33599 loss=0.0, accuracy=1.0\n",
            "Test loss = 24.94114875793457, Test accuracy = 0.5101449489593506\n",
            "Step=33699 loss=0.0, accuracy=1.0\n",
            "Test loss = 24.949888229370117, Test accuracy = 0.5101449489593506\n",
            "Step=33799 loss=0.0, accuracy=1.0\n",
            "Test loss = 24.958921432495117, Test accuracy = 0.5101449489593506\n",
            "Step=33899 loss=0.0, accuracy=1.0\n",
            "Test loss = 24.9685001373291, Test accuracy = 0.5101449489593506\n",
            "Step=33999 loss=0.0, accuracy=1.0\n",
            "Test loss = 24.97869873046875, Test accuracy = 0.5101449489593506\n",
            "Step=34099 loss=0.0, accuracy=1.0\n",
            "Test loss = 24.989341735839844, Test accuracy = 0.5101449489593506\n",
            "Step=34199 loss=0.0, accuracy=1.0\n",
            "Test loss = 24.99967384338379, Test accuracy = 0.5101449489593506\n",
            "Step=34299 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.01038360595703, Test accuracy = 0.5101449489593506\n",
            "Step=34399 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.02143669128418, Test accuracy = 0.5101449489593506\n",
            "Step=34499 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.033864974975586, Test accuracy = 0.5101449489593506\n",
            "Step=34599 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.04543685913086, Test accuracy = 0.5101449489593506\n",
            "Step=34699 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.056941986083984, Test accuracy = 0.5101449489593506\n",
            "Step=34799 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.070037841796875, Test accuracy = 0.5101449489593506\n",
            "Step=34899 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.084089279174805, Test accuracy = 0.5101449489593506\n",
            "Step=34999 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.098716735839844, Test accuracy = 0.5101449489593506\n",
            "Step=35099 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.113262176513672, Test accuracy = 0.5101449489593506\n",
            "Step=35199 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.12879180908203, Test accuracy = 0.5101449489593506\n",
            "Step=35299 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.143918991088867, Test accuracy = 0.5130434632301331\n",
            "Step=35399 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.15903091430664, Test accuracy = 0.5130434632301331\n",
            "Step=35499 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.175342559814453, Test accuracy = 0.5130434632301331\n",
            "Step=35599 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.192020416259766, Test accuracy = 0.5130434632301331\n",
            "Step=35699 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.208820343017578, Test accuracy = 0.5130434632301331\n",
            "Step=35799 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.226091384887695, Test accuracy = 0.5130434632301331\n",
            "Step=35899 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.244277954101562, Test accuracy = 0.5130434632301331\n",
            "Step=35999 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.263111114501953, Test accuracy = 0.5130434632301331\n",
            "Step=36099 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.280900955200195, Test accuracy = 0.5130434632301331\n",
            "Step=36199 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.301158905029297, Test accuracy = 0.5130434632301331\n",
            "Step=36299 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.321645736694336, Test accuracy = 0.5130434632301331\n",
            "Step=36399 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.34170913696289, Test accuracy = 0.5130434632301331\n",
            "Step=36499 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.361591339111328, Test accuracy = 0.5101449489593506\n",
            "Step=36599 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.38104820251465, Test accuracy = 0.5101449489593506\n",
            "Step=36699 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.402681350708008, Test accuracy = 0.5101449489593506\n",
            "Step=36799 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.426250457763672, Test accuracy = 0.5101449489593506\n",
            "Step=36899 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.448060989379883, Test accuracy = 0.5101449489593506\n",
            "Step=36999 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.46979522705078, Test accuracy = 0.5101449489593506\n",
            "Step=37099 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.491344451904297, Test accuracy = 0.5101449489593506\n",
            "Step=37199 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.514802932739258, Test accuracy = 0.5101449489593506\n",
            "Step=37299 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.53907012939453, Test accuracy = 0.5101449489593506\n",
            "Step=37399 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.563297271728516, Test accuracy = 0.5101449489593506\n",
            "Step=37499 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.589099884033203, Test accuracy = 0.5072463750839233\n",
            "Step=37599 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.614917755126953, Test accuracy = 0.5072463750839233\n",
            "Step=37699 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.64099884033203, Test accuracy = 0.5072463750839233\n",
            "Step=37799 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.666303634643555, Test accuracy = 0.5072463750839233\n",
            "Step=37899 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.692001342773438, Test accuracy = 0.5072463750839233\n",
            "Step=37999 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.721981048583984, Test accuracy = 0.5072463750839233\n",
            "Step=38099 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.748960494995117, Test accuracy = 0.5072463750839233\n",
            "Step=38199 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.775381088256836, Test accuracy = 0.5072463750839233\n",
            "Step=38299 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.805561065673828, Test accuracy = 0.5072463750839233\n",
            "Step=38399 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.83448028564453, Test accuracy = 0.5072463750839233\n",
            "Step=38499 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.862844467163086, Test accuracy = 0.5072463750839233\n",
            "Step=38599 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.894168853759766, Test accuracy = 0.5072463750839233\n",
            "Step=38699 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.9224796295166, Test accuracy = 0.5072463750839233\n",
            "Step=38799 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.95513916015625, Test accuracy = 0.5072463750839233\n",
            "Step=38899 loss=0.0, accuracy=1.0\n",
            "Test loss = 25.98689842224121, Test accuracy = 0.5072463750839233\n",
            "Step=38999 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.020902633666992, Test accuracy = 0.5072463750839233\n",
            "Step=39099 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.05445098876953, Test accuracy = 0.5072463750839233\n",
            "Step=39199 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.086275100708008, Test accuracy = 0.5072463750839233\n",
            "Step=39299 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.116806030273438, Test accuracy = 0.5072463750839233\n",
            "Step=39399 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.15109634399414, Test accuracy = 0.5101449489593506\n",
            "Step=39499 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.183269500732422, Test accuracy = 0.5101449489593506\n",
            "Step=39599 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.217140197753906, Test accuracy = 0.5101449489593506\n",
            "Step=39699 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.251798629760742, Test accuracy = 0.5101449489593506\n",
            "Step=39799 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.28568458557129, Test accuracy = 0.5101449489593506\n",
            "Step=39899 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.322158813476562, Test accuracy = 0.5101449489593506\n",
            "Step=39999 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.36034393310547, Test accuracy = 0.5101449489593506\n",
            "Step=40099 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.398237228393555, Test accuracy = 0.5101449489593506\n",
            "Step=40199 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.43763542175293, Test accuracy = 0.5101449489593506\n",
            "Step=40299 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.477264404296875, Test accuracy = 0.5101449489593506\n",
            "Step=40399 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.516685485839844, Test accuracy = 0.5101449489593506\n",
            "Step=40499 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.556081771850586, Test accuracy = 0.5101449489593506\n",
            "Step=40599 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.598031997680664, Test accuracy = 0.5101449489593506\n",
            "Step=40699 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.641826629638672, Test accuracy = 0.5101449489593506\n",
            "Step=40799 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.689239501953125, Test accuracy = 0.5101449489593506\n",
            "Step=40899 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.73456573486328, Test accuracy = 0.5101449489593506\n",
            "Step=40999 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.781665802001953, Test accuracy = 0.5101449489593506\n",
            "Step=41099 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.832324981689453, Test accuracy = 0.5101449489593506\n",
            "Step=41199 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.884328842163086, Test accuracy = 0.5101449489593506\n",
            "Step=41299 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.9338436126709, Test accuracy = 0.5101449489593506\n",
            "Step=41399 loss=0.0, accuracy=1.0\n",
            "Test loss = 26.990102767944336, Test accuracy = 0.5101449489593506\n",
            "Step=41499 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.04819107055664, Test accuracy = 0.5101449489593506\n",
            "Step=41599 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.100814819335938, Test accuracy = 0.5101449489593506\n",
            "Step=41699 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.15802574157715, Test accuracy = 0.5101449489593506\n",
            "Step=41799 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.21912956237793, Test accuracy = 0.5101449489593506\n",
            "Step=41899 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.278610229492188, Test accuracy = 0.5101449489593506\n",
            "Step=41999 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.34563446044922, Test accuracy = 0.5101449489593506\n",
            "Step=42099 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.411365509033203, Test accuracy = 0.5101449489593506\n",
            "Step=42199 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.48093605041504, Test accuracy = 0.5130434632301331\n",
            "Step=42299 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.552898406982422, Test accuracy = 0.5130434632301331\n",
            "Step=42399 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.623472213745117, Test accuracy = 0.5101449489593506\n",
            "Step=42499 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.700231552124023, Test accuracy = 0.5130434632301331\n",
            "Step=42599 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.77985191345215, Test accuracy = 0.5130434632301331\n",
            "Step=42699 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.85731315612793, Test accuracy = 0.5130434632301331\n",
            "Step=42799 loss=0.0, accuracy=1.0\n",
            "Test loss = 27.941730499267578, Test accuracy = 0.5130434632301331\n",
            "Step=42899 loss=0.0, accuracy=1.0\n",
            "Test loss = 28.027406692504883, Test accuracy = 0.5130434632301331\n",
            "Step=42999 loss=0.0, accuracy=1.0\n",
            "Test loss = 28.119529724121094, Test accuracy = 0.5130434632301331\n",
            "Step=43099 loss=0.0, accuracy=1.0\n",
            "Test loss = 28.208133697509766, Test accuracy = 0.5130434632301331\n",
            "Step=43199 loss=0.0, accuracy=1.0\n",
            "Test loss = 28.30213165283203, Test accuracy = 0.5130434632301331\n",
            "Step=43299 loss=0.0, accuracy=1.0\n",
            "Test loss = 28.396129608154297, Test accuracy = 0.5101449489593506\n",
            "Step=43399 loss=0.0, accuracy=1.0\n",
            "Test loss = 28.498918533325195, Test accuracy = 0.5101449489593506\n",
            "Step=43499 loss=0.0, accuracy=1.0\n",
            "Test loss = 28.61155891418457, Test accuracy = 0.5101449489593506\n",
            "Step=43599 loss=0.0, accuracy=1.0\n",
            "Test loss = 28.71660614013672, Test accuracy = 0.5101449489593506\n",
            "Step=43699 loss=0.0, accuracy=1.0\n",
            "Test loss = 28.832569122314453, Test accuracy = 0.5101449489593506\n",
            "Step=43799 loss=0.0, accuracy=1.0\n",
            "Test loss = 28.961687088012695, Test accuracy = 0.5101449489593506\n",
            "Step=43899 loss=0.0, accuracy=1.0\n",
            "Test loss = 29.084505081176758, Test accuracy = 0.5101449489593506\n",
            "Step=43999 loss=0.0, accuracy=1.0\n",
            "Test loss = 29.215238571166992, Test accuracy = 0.5130434632301331\n",
            "Step=44099 loss=0.0, accuracy=1.0\n",
            "Test loss = 29.34164810180664, Test accuracy = 0.5101449489593506\n",
            "Step=44199 loss=0.0, accuracy=1.0\n",
            "Test loss = 29.48163604736328, Test accuracy = 0.5130434632301331\n",
            "Step=44299 loss=0.0, accuracy=1.0\n",
            "Test loss = 29.624900817871094, Test accuracy = 0.5130434632301331\n",
            "Step=44399 loss=0.0, accuracy=1.0\n",
            "Test loss = 29.78254508972168, Test accuracy = 0.5130434632301331\n",
            "Step=44499 loss=0.0, accuracy=1.0\n",
            "Test loss = 29.9465274810791, Test accuracy = 0.5130434632301331\n",
            "Step=44599 loss=0.0, accuracy=1.0\n",
            "Test loss = 30.115352630615234, Test accuracy = 0.5101449489593506\n",
            "Step=44699 loss=0.0, accuracy=1.0\n",
            "Test loss = 30.291017532348633, Test accuracy = 0.5130434632301331\n",
            "Step=44799 loss=0.0, accuracy=1.0\n",
            "Test loss = 30.47159194946289, Test accuracy = 0.5130434632301331\n",
            "Step=44899 loss=0.0, accuracy=1.0\n",
            "Test loss = 30.679248809814453, Test accuracy = 0.5130434632301331\n",
            "Step=44999 loss=0.0, accuracy=1.0\n",
            "Test loss = 30.920860290527344, Test accuracy = 0.5101449489593506\n",
            "Step=45099 loss=0.0, accuracy=1.0\n",
            "Test loss = 31.190095901489258, Test accuracy = 0.5101449489593506\n",
            "Step=45199 loss=0.0, accuracy=1.0\n",
            "Test loss = 31.454923629760742, Test accuracy = 0.5130434632301331\n",
            "Step=45299 loss=0.0, accuracy=1.0\n",
            "Test loss = 31.738861083984375, Test accuracy = 0.5130434632301331\n",
            "Step=45399 loss=0.0, accuracy=1.0\n",
            "Test loss = 32.06856918334961, Test accuracy = 0.5130434632301331\n",
            "Step=45499 loss=0.0, accuracy=1.0\n",
            "Test loss = 32.4212532043457, Test accuracy = 0.5130434632301331\n",
            "Step=45599 loss=0.0, accuracy=1.0\n",
            "Test loss = 32.8094367980957, Test accuracy = 0.5130434632301331\n",
            "Step=45699 loss=0.0, accuracy=1.0\n",
            "Test loss = 33.218292236328125, Test accuracy = 0.5130434632301331\n",
            "Step=45799 loss=0.0, accuracy=1.0\n",
            "Test loss = 33.66375732421875, Test accuracy = 0.5130434632301331\n",
            "Step=45899 loss=0.0, accuracy=1.0\n",
            "Test loss = 34.120845794677734, Test accuracy = 0.5130434632301331\n",
            "Step=45999 loss=0.0, accuracy=1.0\n",
            "Test loss = 34.60966110229492, Test accuracy = 0.5130434632301331\n",
            "Step=46099 loss=0.0, accuracy=1.0\n",
            "Test loss = 35.08058166503906, Test accuracy = 0.5130434632301331\n",
            "Step=46199 loss=0.0, accuracy=1.0\n",
            "Test loss = 35.611080169677734, Test accuracy = 0.5130434632301331\n",
            "Step=46299 loss=0.0, accuracy=1.0\n",
            "Test loss = 36.16604995727539, Test accuracy = 0.5130434632301331\n",
            "Step=46399 loss=0.0, accuracy=1.0\n",
            "Test loss = 36.72774887084961, Test accuracy = 0.5130434632301331\n",
            "Step=46499 loss=0.0, accuracy=1.0\n",
            "Test loss = 37.347633361816406, Test accuracy = 0.5130434632301331\n",
            "Step=46599 loss=0.0, accuracy=1.0\n",
            "Test loss = 37.91685485839844, Test accuracy = 0.5101449489593506\n",
            "Step=46699 loss=0.0, accuracy=1.0\n",
            "Test loss = 38.527427673339844, Test accuracy = 0.5101449489593506\n",
            "Step=46799 loss=0.0, accuracy=1.0\n",
            "Test loss = 39.1678581237793, Test accuracy = 0.5101449489593506\n",
            "Step=46899 loss=0.0, accuracy=1.0\n",
            "Test loss = 39.826499938964844, Test accuracy = 0.5130434632301331\n",
            "Step=46999 loss=0.0, accuracy=1.0\n",
            "Test loss = 40.56888198852539, Test accuracy = 0.5159420371055603\n",
            "Step=47099 loss=0.0, accuracy=1.0\n",
            "Test loss = 41.33463668823242, Test accuracy = 0.5130434632301331\n",
            "Step=47199 loss=0.0, accuracy=1.0\n",
            "Test loss = 42.06620407104492, Test accuracy = 0.5130434632301331\n",
            "Step=47299 loss=0.0, accuracy=1.0\n",
            "Test loss = 42.79124069213867, Test accuracy = 0.5159420371055603\n",
            "Step=47399 loss=0.0, accuracy=1.0\n",
            "Test loss = 43.43642044067383, Test accuracy = 0.5159420371055603\n",
            "Step=47499 loss=0.0, accuracy=1.0\n",
            "Test loss = 43.95521926879883, Test accuracy = 0.5159420371055603\n",
            "Step=47599 loss=0.0, accuracy=1.0\n",
            "Test loss = 44.46334457397461, Test accuracy = 0.5159420371055603\n",
            "Step=47699 loss=0.0, accuracy=1.0\n",
            "Test loss = 44.88064193725586, Test accuracy = 0.5159420371055603\n",
            "Step=47799 loss=0.0, accuracy=1.0\n",
            "Test loss = 45.33284378051758, Test accuracy = 0.5159420371055603\n",
            "Step=47899 loss=0.0, accuracy=1.0\n",
            "Test loss = 45.76488494873047, Test accuracy = 0.5159420371055603\n",
            "Step=47999 loss=0.0, accuracy=1.0\n",
            "Test loss = 46.19548797607422, Test accuracy = 0.5159420371055603\n",
            "Step=48099 loss=0.0, accuracy=1.0\n",
            "Test loss = 46.63998794555664, Test accuracy = 0.5159420371055603\n",
            "Step=48199 loss=0.0, accuracy=1.0\n",
            "Test loss = 47.16612243652344, Test accuracy = 0.5159420371055603\n",
            "Step=48299 loss=0.0, accuracy=1.0\n",
            "Test loss = 47.82405090332031, Test accuracy = 0.5130434632301331\n",
            "Step=48399 loss=0.0, accuracy=1.0\n",
            "Test loss = 49.43331527709961, Test accuracy = 0.5130434632301331\n",
            "Step=48499 loss=0.09905200977941821, accuracy=0.990859375\n",
            "Test loss = 54.49635696411133, Test accuracy = 0.5072463750839233\n",
            "Step=48599 loss=5.850932481621385e-06, accuracy=1.0\n",
            "Test loss = 54.29597854614258, Test accuracy = 0.5072463750839233\n",
            "Step=48699 loss=1.7425601025422565e-06, accuracy=1.0\n",
            "Test loss = 54.2601318359375, Test accuracy = 0.5072463750839233\n",
            "Step=48799 loss=1.179326141027559e-06, accuracy=1.0\n",
            "Test loss = 54.237064361572266, Test accuracy = 0.5072463750839233\n",
            "Step=48899 loss=1.0682436064257672e-06, accuracy=1.0\n",
            "Test loss = 54.217613220214844, Test accuracy = 0.5072463750839233\n",
            "Step=48999 loss=8.212334823554101e-07, accuracy=1.0\n",
            "Test loss = 54.20390701293945, Test accuracy = 0.5072463750839233\n",
            "Step=49099 loss=7.018586134677207e-07, accuracy=1.0\n",
            "Test loss = 54.192867279052734, Test accuracy = 0.5072463750839233\n",
            "Step=49199 loss=5.835634082984597e-07, accuracy=1.0\n",
            "Test loss = 54.18436050415039, Test accuracy = 0.5072463750839233\n",
            "Step=49299 loss=5.706370958336037e-07, accuracy=1.0\n",
            "Test loss = 54.1755256652832, Test accuracy = 0.5072463750839233\n",
            "Step=49399 loss=4.931517558759424e-07, accuracy=1.0\n",
            "Test loss = 54.16840744018555, Test accuracy = 0.5072463750839233\n",
            "Step=49499 loss=4.745628780256084e-07, accuracy=1.0\n",
            "Test loss = 54.161556243896484, Test accuracy = 0.5072463750839233\n",
            "Step=49599 loss=4.4561764283912453e-07, accuracy=1.0\n",
            "Test loss = 54.15520477294922, Test accuracy = 0.5072463750839233\n",
            "Step=49699 loss=4.217201305323215e-07, accuracy=1.0\n",
            "Test loss = 54.14931106567383, Test accuracy = 0.5072463750839233\n",
            "Step=49799 loss=3.5629502157519257e-07, accuracy=1.0\n",
            "Test loss = 54.14435958862305, Test accuracy = 0.5072463750839233\n",
            "Step=49899 loss=3.5353838072182954e-07, accuracy=1.0\n",
            "Test loss = 54.13949203491211, Test accuracy = 0.5072463750839233\n",
            "Step=49999 loss=3.552893234370913e-07, accuracy=1.0\n",
            "Test loss = 54.13463592529297, Test accuracy = 0.5072463750839233\n",
            "Reached 50001 epochs for CNN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yjbCWx-nnW7i",
        "outputId": "bb2944b5-65d3-4c49-a7a6-f8db1e1931a5"
      },
      "source": [
        "#train XGB Boost\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "# from gan import GAN\r\n",
        "import random\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n",
        "import xgboost as xgb\r\n",
        "from sklearn.externals import joblib\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "# from plot_confusion_matrix import plot_confusion_matrix\r\n",
        "\r\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\r\n",
        "\r\n",
        "class TrainXGBBoost:\r\n",
        "\r\n",
        "    def __init__(self, num_historical_days, days=10, pct_change=0):\r\n",
        "        self.data = []\r\n",
        "        self.labels = []\r\n",
        "        self.test_data = []\r\n",
        "        self.test_labels = []\r\n",
        "        \r\n",
        "        assert os.path.exists(f'{googlepath}models/checkpoint')\r\n",
        "        gan = GAN(num_features=5, num_historical_days=num_historical_days,\r\n",
        "                        generator_input_size=200, is_train=False)\r\n",
        "        with tf.Session() as sess:\r\n",
        "            sess.run(tf.global_variables_initializer())\r\n",
        "            saver = tf.train.Saver()\r\n",
        "            if os.path.exists(f'{googlepath}models/checkpoint'):\r\n",
        "                    \r\n",
        "                    with open(f'{googlepath}models/checkpoint', 'rb') as f:\r\n",
        "                        model_name = next(f).split('\"'.encode())[1]\r\n",
        "                    filename = \"{}models/{}\".format(googlepath, model_name.decode())\r\n",
        "                    currentStep = filename.split(\"-\")[1]\r\n",
        "                    new_saver = tf.train.import_meta_graph('{}.meta'.format(filename))\r\n",
        "                    new_saver.restore(sess, \"{}\".format(filename))\r\n",
        "            files = [os.path.join(f'{googlepath}stock_data', f) for f in os.listdir(f'{googlepath}/stock_data')]\r\n",
        "            for file in files:\r\n",
        "                print(file)\r\n",
        "                #Read in file -- note that parse_dates will be need later\r\n",
        "                df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\r\n",
        "                df = df[['open','high','low','close','volume']]\r\n",
        "                # #Create new index with missing days\r\n",
        "                # idx = pd.date_range(df.index[-1], df.index[0])\r\n",
        "                # #Reindex and fill the missing day with the value from the day before\r\n",
        "                # df = df.reindex(idx, method='bfill').sort_index(ascending=False)\r\n",
        "                #Normilize using a of size num_historical_days\r\n",
        "                labels = df.close.pct_change(days).map(lambda x: int(x > pct_change/100.0))\r\n",
        "                df = ((df -\r\n",
        "                df.rolling(num_historical_days).mean().shift(-num_historical_days))\r\n",
        "                /(df.rolling(num_historical_days).max().shift(-num_historical_days)\r\n",
        "                -df.rolling(num_historical_days).min().shift(-num_historical_days)))\r\n",
        "                df['labels'] = labels\r\n",
        "                #Drop the last 10 day that we don't have data for\r\n",
        "                df = df.dropna()\r\n",
        "                #Hold out the last year of trading for testing\r\n",
        "                test_df = df[:365]\r\n",
        "                #Padding to keep labels from bleeding\r\n",
        "                df = df[400:]\r\n",
        "                #This may not create good samples if num_historical_days is a\r\n",
        "                #mutliple of 7\r\n",
        "                data = df[['open','high','low','close','volume']].values\r\n",
        "                labels = df['labels'].values\r\n",
        "                for i in range(num_historical_days, len(df), num_historical_days):\r\n",
        "                    features = sess.run(gan.features, feed_dict={gan.X:[data[i-num_historical_days:i]]})\r\n",
        "                    self.data.append(features[0])\r\n",
        "#                     print(features[0])\r\n",
        "                    self.labels.append(labels[i-1])\r\n",
        "                data = test_df[['open','high','low','close','volume']].values\r\n",
        "                labels = test_df['labels'].values\r\n",
        "                for i in range(num_historical_days, len(test_df), 1):\r\n",
        "                    features = sess.run(gan.features, feed_dict={gan.X:[data[i-num_historical_days:i]]})\r\n",
        "                    self.test_data.append(features[0])\r\n",
        "                    self.test_labels.append(labels[i-1])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def train(self):\r\n",
        "        params = {}\r\n",
        "        params['objective'] = 'multi:softprob'\r\n",
        "        params['eta'] = 0.01 # The training step in each iteration\r\n",
        "        params['num_class'] = 2 # Number of classe in the dataset\r\n",
        "        params['max_depth'] = 20 # Max depth in each tree\r\n",
        "        params['subsample'] = 0.05 \r\n",
        "        params['colsample_bytree'] = 0.05\r\n",
        "        params['eval_metric'] = 'mlogloss' \r\n",
        "        #params['scale_pos_weight'] = 10\r\n",
        "        #params['silent'] = True\r\n",
        "        #params['gpu_id'] = 0\r\n",
        "        #params['max_bin'] = 16\r\n",
        "        #params['tree_method'] = 'gpu_hist'\r\n",
        "\r\n",
        "        train = xgb.DMatrix(self.data, self.labels)\r\n",
        "        test = xgb.DMatrix(self.test_data, self.test_labels)\r\n",
        "\r\n",
        "        watchlist = [(train, 'train'), (test, 'test')]\r\n",
        "        clf = xgb.train(params, train, 1000, evals=watchlist, early_stopping_rounds=100)\r\n",
        "        joblib.dump(clf, f'{googlepath}models/clf.pkl')\r\n",
        "        # cm = confusion_matrix(self.test_labels, map(lambda x: int(x[1] > .5), clf.predict(test)))\r\n",
        "        cm = confusion_matrix(self.test_labels, list(map(lambda x: int(x[1] > .5), clf.predict(test)))) # \r\n",
        "        print(cm)\r\n",
        "        plot_confusion_matrix(cm, ['Down', 'Up'], normalize=True, title=\"Confusion Matrix\")\r\n",
        "\r\n",
        "tf.reset_default_graph()\r\n",
        "boost_model = TrainXGBBoost(num_historical_days=HISTORICAL_DAYS_AMOUNT, days=DAYS_AHEAD, pct_change=PCT_CHANGE_AMOUNT)\r\n",
        "boost_model.train()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Relu:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_2:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"Relu_4:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_5:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_6:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/Colab Notebooks/GAN cryptocurrency/models/gan.ckpt-49999\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BTC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/LTC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/NMC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/PPC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/ETH.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BNB.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/DOT.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/ADA.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/USDT.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XRP.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/LINK.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BCH.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XLM.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/UNI.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/USDC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/DOGE.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/WBTC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/AAVE.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/ATOM.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/EOS.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XEM.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XMR.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BSV.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/TRX.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/HT .csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/MIOTA.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XTZ.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/VET.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/THETA.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/CRO.csv\n",
            "[0]\ttrain-mlogloss:0.68742\ttest-mlogloss:0.693118\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 100 rounds.\n",
            "[1]\ttrain-mlogloss:0.682804\ttest-mlogloss:0.693355\n",
            "[2]\ttrain-mlogloss:0.677464\ttest-mlogloss:0.693685\n",
            "[3]\ttrain-mlogloss:0.672779\ttest-mlogloss:0.693069\n",
            "[4]\ttrain-mlogloss:0.666831\ttest-mlogloss:0.692897\n",
            "[5]\ttrain-mlogloss:0.661403\ttest-mlogloss:0.692202\n",
            "[6]\ttrain-mlogloss:0.655974\ttest-mlogloss:0.692973\n",
            "[7]\ttrain-mlogloss:0.651583\ttest-mlogloss:0.692614\n",
            "[8]\ttrain-mlogloss:0.646435\ttest-mlogloss:0.692368\n",
            "[9]\ttrain-mlogloss:0.642594\ttest-mlogloss:0.691875\n",
            "[10]\ttrain-mlogloss:0.638646\ttest-mlogloss:0.691634\n",
            "[11]\ttrain-mlogloss:0.633033\ttest-mlogloss:0.692068\n",
            "[12]\ttrain-mlogloss:0.627254\ttest-mlogloss:0.691427\n",
            "[13]\ttrain-mlogloss:0.623101\ttest-mlogloss:0.691741\n",
            "[14]\ttrain-mlogloss:0.61819\ttest-mlogloss:0.691445\n",
            "[15]\ttrain-mlogloss:0.614433\ttest-mlogloss:0.691311\n",
            "[16]\ttrain-mlogloss:0.608331\ttest-mlogloss:0.690649\n",
            "[17]\ttrain-mlogloss:0.604834\ttest-mlogloss:0.690453\n",
            "[18]\ttrain-mlogloss:0.600595\ttest-mlogloss:0.690568\n",
            "[19]\ttrain-mlogloss:0.594969\ttest-mlogloss:0.69003\n",
            "[20]\ttrain-mlogloss:0.589694\ttest-mlogloss:0.689468\n",
            "[21]\ttrain-mlogloss:0.586072\ttest-mlogloss:0.689813\n",
            "[22]\ttrain-mlogloss:0.582003\ttest-mlogloss:0.689489\n",
            "[23]\ttrain-mlogloss:0.577346\ttest-mlogloss:0.689208\n",
            "[24]\ttrain-mlogloss:0.572891\ttest-mlogloss:0.68964\n",
            "[25]\ttrain-mlogloss:0.568651\ttest-mlogloss:0.689442\n",
            "[26]\ttrain-mlogloss:0.565004\ttest-mlogloss:0.689563\n",
            "[27]\ttrain-mlogloss:0.561257\ttest-mlogloss:0.690051\n",
            "[28]\ttrain-mlogloss:0.557202\ttest-mlogloss:0.690066\n",
            "[29]\ttrain-mlogloss:0.554739\ttest-mlogloss:0.690301\n",
            "[30]\ttrain-mlogloss:0.551688\ttest-mlogloss:0.69036\n",
            "[31]\ttrain-mlogloss:0.548417\ttest-mlogloss:0.690146\n",
            "[32]\ttrain-mlogloss:0.54476\ttest-mlogloss:0.689867\n",
            "[33]\ttrain-mlogloss:0.540875\ttest-mlogloss:0.689886\n",
            "[34]\ttrain-mlogloss:0.536468\ttest-mlogloss:0.689718\n",
            "[35]\ttrain-mlogloss:0.531812\ttest-mlogloss:0.6895\n",
            "[36]\ttrain-mlogloss:0.528725\ttest-mlogloss:0.690059\n",
            "[37]\ttrain-mlogloss:0.525312\ttest-mlogloss:0.68976\n",
            "[38]\ttrain-mlogloss:0.521041\ttest-mlogloss:0.690098\n",
            "[39]\ttrain-mlogloss:0.517677\ttest-mlogloss:0.689612\n",
            "[40]\ttrain-mlogloss:0.515117\ttest-mlogloss:0.688685\n",
            "[41]\ttrain-mlogloss:0.510806\ttest-mlogloss:0.688995\n",
            "[42]\ttrain-mlogloss:0.50787\ttest-mlogloss:0.689138\n",
            "[43]\ttrain-mlogloss:0.504621\ttest-mlogloss:0.689611\n",
            "[44]\ttrain-mlogloss:0.501562\ttest-mlogloss:0.688824\n",
            "[45]\ttrain-mlogloss:0.497562\ttest-mlogloss:0.688929\n",
            "[46]\ttrain-mlogloss:0.493969\ttest-mlogloss:0.688988\n",
            "[47]\ttrain-mlogloss:0.490586\ttest-mlogloss:0.68902\n",
            "[48]\ttrain-mlogloss:0.487606\ttest-mlogloss:0.688493\n",
            "[49]\ttrain-mlogloss:0.483588\ttest-mlogloss:0.688437\n",
            "[50]\ttrain-mlogloss:0.479921\ttest-mlogloss:0.68844\n",
            "[51]\ttrain-mlogloss:0.477333\ttest-mlogloss:0.688368\n",
            "[52]\ttrain-mlogloss:0.473717\ttest-mlogloss:0.688666\n",
            "[53]\ttrain-mlogloss:0.470909\ttest-mlogloss:0.688938\n",
            "[54]\ttrain-mlogloss:0.468548\ttest-mlogloss:0.689462\n",
            "[55]\ttrain-mlogloss:0.465026\ttest-mlogloss:0.689501\n",
            "[56]\ttrain-mlogloss:0.461479\ttest-mlogloss:0.689351\n",
            "[57]\ttrain-mlogloss:0.458889\ttest-mlogloss:0.689264\n",
            "[58]\ttrain-mlogloss:0.455553\ttest-mlogloss:0.688721\n",
            "[59]\ttrain-mlogloss:0.452232\ttest-mlogloss:0.688512\n",
            "[60]\ttrain-mlogloss:0.449373\ttest-mlogloss:0.688852\n",
            "[61]\ttrain-mlogloss:0.445884\ttest-mlogloss:0.688953\n",
            "[62]\ttrain-mlogloss:0.44273\ttest-mlogloss:0.689508\n",
            "[63]\ttrain-mlogloss:0.439507\ttest-mlogloss:0.689697\n",
            "[64]\ttrain-mlogloss:0.436359\ttest-mlogloss:0.689943\n",
            "[65]\ttrain-mlogloss:0.433827\ttest-mlogloss:0.689274\n",
            "[66]\ttrain-mlogloss:0.431063\ttest-mlogloss:0.689596\n",
            "[67]\ttrain-mlogloss:0.42797\ttest-mlogloss:0.689401\n",
            "[68]\ttrain-mlogloss:0.425729\ttest-mlogloss:0.690346\n",
            "[69]\ttrain-mlogloss:0.423257\ttest-mlogloss:0.689905\n",
            "[70]\ttrain-mlogloss:0.420404\ttest-mlogloss:0.689306\n",
            "[71]\ttrain-mlogloss:0.418633\ttest-mlogloss:0.689215\n",
            "[72]\ttrain-mlogloss:0.416329\ttest-mlogloss:0.688789\n",
            "[73]\ttrain-mlogloss:0.413699\ttest-mlogloss:0.68881\n",
            "[74]\ttrain-mlogloss:0.410916\ttest-mlogloss:0.688907\n",
            "[75]\ttrain-mlogloss:0.407858\ttest-mlogloss:0.688707\n",
            "[76]\ttrain-mlogloss:0.405272\ttest-mlogloss:0.688823\n",
            "[77]\ttrain-mlogloss:0.403277\ttest-mlogloss:0.6884\n",
            "[78]\ttrain-mlogloss:0.401021\ttest-mlogloss:0.688604\n",
            "[79]\ttrain-mlogloss:0.398799\ttest-mlogloss:0.688655\n",
            "[80]\ttrain-mlogloss:0.396193\ttest-mlogloss:0.688411\n",
            "[81]\ttrain-mlogloss:0.393896\ttest-mlogloss:0.688978\n",
            "[82]\ttrain-mlogloss:0.391145\ttest-mlogloss:0.688531\n",
            "[83]\ttrain-mlogloss:0.389101\ttest-mlogloss:0.688673\n",
            "[84]\ttrain-mlogloss:0.386765\ttest-mlogloss:0.68875\n",
            "[85]\ttrain-mlogloss:0.383999\ttest-mlogloss:0.689073\n",
            "[86]\ttrain-mlogloss:0.381364\ttest-mlogloss:0.689583\n",
            "[87]\ttrain-mlogloss:0.378799\ttest-mlogloss:0.689606\n",
            "[88]\ttrain-mlogloss:0.376178\ttest-mlogloss:0.68975\n",
            "[89]\ttrain-mlogloss:0.373579\ttest-mlogloss:0.689515\n",
            "[90]\ttrain-mlogloss:0.371502\ttest-mlogloss:0.690107\n",
            "[91]\ttrain-mlogloss:0.369296\ttest-mlogloss:0.69015\n",
            "[92]\ttrain-mlogloss:0.366833\ttest-mlogloss:0.690625\n",
            "[93]\ttrain-mlogloss:0.364335\ttest-mlogloss:0.690436\n",
            "[94]\ttrain-mlogloss:0.361901\ttest-mlogloss:0.690021\n",
            "[95]\ttrain-mlogloss:0.359652\ttest-mlogloss:0.690055\n",
            "[96]\ttrain-mlogloss:0.357926\ttest-mlogloss:0.689865\n",
            "[97]\ttrain-mlogloss:0.355274\ttest-mlogloss:0.689993\n",
            "[98]\ttrain-mlogloss:0.353149\ttest-mlogloss:0.690451\n",
            "[99]\ttrain-mlogloss:0.351349\ttest-mlogloss:0.69072\n",
            "[100]\ttrain-mlogloss:0.34904\ttest-mlogloss:0.690811\n",
            "[101]\ttrain-mlogloss:0.346916\ttest-mlogloss:0.690741\n",
            "[102]\ttrain-mlogloss:0.344688\ttest-mlogloss:0.690838\n",
            "[103]\ttrain-mlogloss:0.342818\ttest-mlogloss:0.691177\n",
            "[104]\ttrain-mlogloss:0.341191\ttest-mlogloss:0.691225\n",
            "[105]\ttrain-mlogloss:0.33935\ttest-mlogloss:0.691036\n",
            "[106]\ttrain-mlogloss:0.33714\ttest-mlogloss:0.691179\n",
            "[107]\ttrain-mlogloss:0.335194\ttest-mlogloss:0.690955\n",
            "[108]\ttrain-mlogloss:0.333283\ttest-mlogloss:0.690861\n",
            "[109]\ttrain-mlogloss:0.331169\ttest-mlogloss:0.691155\n",
            "[110]\ttrain-mlogloss:0.329434\ttest-mlogloss:0.69141\n",
            "[111]\ttrain-mlogloss:0.327503\ttest-mlogloss:0.691622\n",
            "[112]\ttrain-mlogloss:0.326206\ttest-mlogloss:0.691735\n",
            "[113]\ttrain-mlogloss:0.324761\ttest-mlogloss:0.691875\n",
            "[114]\ttrain-mlogloss:0.323403\ttest-mlogloss:0.691765\n",
            "[115]\ttrain-mlogloss:0.321318\ttest-mlogloss:0.691262\n",
            "[116]\ttrain-mlogloss:0.319659\ttest-mlogloss:0.691397\n",
            "[117]\ttrain-mlogloss:0.317817\ttest-mlogloss:0.69136\n",
            "[118]\ttrain-mlogloss:0.315834\ttest-mlogloss:0.69141\n",
            "[119]\ttrain-mlogloss:0.313658\ttest-mlogloss:0.691513\n",
            "[120]\ttrain-mlogloss:0.311728\ttest-mlogloss:0.691427\n",
            "[121]\ttrain-mlogloss:0.309731\ttest-mlogloss:0.691357\n",
            "[122]\ttrain-mlogloss:0.307954\ttest-mlogloss:0.69131\n",
            "[123]\ttrain-mlogloss:0.306097\ttest-mlogloss:0.691491\n",
            "[124]\ttrain-mlogloss:0.304312\ttest-mlogloss:0.69168\n",
            "[125]\ttrain-mlogloss:0.30234\ttest-mlogloss:0.691805\n",
            "[126]\ttrain-mlogloss:0.300276\ttest-mlogloss:0.692103\n",
            "[127]\ttrain-mlogloss:0.29896\ttest-mlogloss:0.691845\n",
            "[128]\ttrain-mlogloss:0.297341\ttest-mlogloss:0.691777\n",
            "[129]\ttrain-mlogloss:0.295588\ttest-mlogloss:0.691851\n",
            "[130]\ttrain-mlogloss:0.294161\ttest-mlogloss:0.691817\n",
            "[131]\ttrain-mlogloss:0.292663\ttest-mlogloss:0.691649\n",
            "[132]\ttrain-mlogloss:0.291243\ttest-mlogloss:0.691425\n",
            "[133]\ttrain-mlogloss:0.289479\ttest-mlogloss:0.691233\n",
            "[134]\ttrain-mlogloss:0.288029\ttest-mlogloss:0.691297\n",
            "[135]\ttrain-mlogloss:0.286514\ttest-mlogloss:0.691447\n",
            "[136]\ttrain-mlogloss:0.284908\ttest-mlogloss:0.691789\n",
            "[137]\ttrain-mlogloss:0.283318\ttest-mlogloss:0.692061\n",
            "[138]\ttrain-mlogloss:0.281896\ttest-mlogloss:0.692461\n",
            "[139]\ttrain-mlogloss:0.280181\ttest-mlogloss:0.692417\n",
            "[140]\ttrain-mlogloss:0.278571\ttest-mlogloss:0.692616\n",
            "[141]\ttrain-mlogloss:0.276923\ttest-mlogloss:0.693427\n",
            "[142]\ttrain-mlogloss:0.275455\ttest-mlogloss:0.693279\n",
            "[143]\ttrain-mlogloss:0.27379\ttest-mlogloss:0.693119\n",
            "[144]\ttrain-mlogloss:0.272392\ttest-mlogloss:0.693373\n",
            "[145]\ttrain-mlogloss:0.270624\ttest-mlogloss:0.693448\n",
            "[146]\ttrain-mlogloss:0.269009\ttest-mlogloss:0.693692\n",
            "[147]\ttrain-mlogloss:0.267576\ttest-mlogloss:0.693387\n",
            "[148]\ttrain-mlogloss:0.266009\ttest-mlogloss:0.693546\n",
            "[149]\ttrain-mlogloss:0.264276\ttest-mlogloss:0.694023\n",
            "[150]\ttrain-mlogloss:0.262799\ttest-mlogloss:0.694232\n",
            "[151]\ttrain-mlogloss:0.261341\ttest-mlogloss:0.693991\n",
            "Stopping. Best iteration:\n",
            "[51]\ttrain-mlogloss:0.477333\ttest-mlogloss:0.688368\n",
            "\n",
            "[[3240 1530]\n",
            " [3150 2430]]\n",
            "Normalized confusion matrix\n",
            "[[0.67924528 0.32075472]\n",
            " [0.56451613 0.43548387]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8ZWQufEnhWA",
        "outputId": "717b629d-a703-41b6-f825-01b4907158f0"
      },
      "source": [
        "#renaming some folders for predictions\r\n",
        "import os\r\n",
        "\r\n",
        "# Below Code Gets Rid of Copy Of when making a copy of the models.\r\n",
        "for filename in os.listdir(f'{googlepath}deployed_model/'):\r\n",
        "    split = filename.split(\" \")\r\n",
        "    if len(split) == 3:\r\n",
        "        os.rename(f'{googlepath}deployed_model/{filename}', f'{googlepath}deployed_model/{split[2]}')\r\n",
        "    else:\r\n",
        "        print(\"Nothing to rename.\")\r\n",
        "\r\n",
        "# Below Code Gets Rid of Checkpoint Number in Name\r\n",
        "for filename in os.listdir(f'{googlepath}deployed_model/'):\r\n",
        "    split = filename.split(\".\")\r\n",
        "    if len(split) == 3:\r\n",
        "        prefix, _, suffix = split\r\n",
        "        new_name = prefix + \".\" + suffix\r\n",
        "        os.rename(f'{googlepath}deployed_model/{filename}', f'{googlepath}deployed_model/{new_name}')\r\n",
        "    elif filename == \"clf.pkl\":\r\n",
        "        os.rename(f'{googlepath}deployed_model/{filename}', f'{googlepath}deployed_model/xgb')\r\n",
        "    else:\r\n",
        "        print(\"Nothing to rename.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nothing to rename.\n",
            "Nothing to rename.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eeTuT-8lpqRW",
        "outputId": "198bee5d-fe2e-4fba-f1f9-cfd2f2dbd1fa"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "# from gan import GAN\r\n",
        "import random\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n",
        "import xgboost as xgb\r\n",
        "from sklearn.externals import joblib\r\n",
        "\r\n",
        "\r\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\r\n",
        "\r\n",
        "class Predict:\r\n",
        "\r\n",
        "    def __init__(self, num_historical_days=20, days=10, pct_change=0, \r\n",
        "                 gan_model=f'{googlepath}deployed_model/gan', \r\n",
        "                 cnn_model=f'{googlepath}deployed_model/cnn', \r\n",
        "                 xgb_model=f'{googlepath}deployed_model/xgb'):\r\n",
        "        self.data = []\r\n",
        "        self.num_historical_days = num_historical_days\r\n",
        "        self.gan_model = gan_model\r\n",
        "        self.cnn_model = cnn_model\r\n",
        "        self.xgb_model = xgb_model\r\n",
        "        # assert os.path.exists(gan_model)\r\n",
        "        # assert os.path.exists(cnn_modle)\r\n",
        "        # assert os.path.exists(xgb_model)\r\n",
        "\r\n",
        "        files = [os.path.join(f'{googlepath}stock_data', f) for f in os.listdir(f'{googlepath}stock_data')]\r\n",
        "        for file in files:\r\n",
        "            print(file)\r\n",
        "            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\r\n",
        "            df = df.sort_index(ascending=False)\r\n",
        "            df = df[['open', 'high', 'low', 'close', 'volume']]\r\n",
        "            df = ((df -\r\n",
        "                  df.rolling(num_historical_days).mean().shift(-num_historical_days)) /\r\n",
        "                  (df.rolling(num_historical_days).max().shift(-num_historical_days) -\r\n",
        "                  df.rolling(num_historical_days).min().shift(-num_historical_days)))\r\n",
        "            df = df.dropna()\r\n",
        "\r\n",
        "            \"\"\"\r\n",
        "            file.split --> is the symbol of the current file. Append a tuple of\r\n",
        "            that symbol and the dataframe index[0] which is the timestamp, and\r\n",
        "            thirdly append the data for 200 to 200 + num_historical_days values\r\n",
        "            (open, high, low, close, volume). For each symbol we have, we are\r\n",
        "            predicting based on the df[200:200+num_historical_days].values...\r\n",
        "            \"\"\"\r\n",
        "            self.data.append((file.split('/')[-1], df.index[0], df[200:200+num_historical_days].values))\r\n",
        "            \r\n",
        "#         print(\"Data: \", self.data)\r\n",
        "\r\n",
        "\r\n",
        "    def gan_predict(self):\r\n",
        "        # clears the default graph stack and resets the global default graph.\r\n",
        "\r\n",
        "        tf.reset_default_graph() \r\n",
        "        gan = GAN(num_features=5, num_historical_days=self.num_historical_days,\r\n",
        "                  generator_input_size=200, is_train=False)\r\n",
        "        # A class for running Tensorflow operations. A session object\r\n",
        "        # encapsulates the environment in which Operation objects are executed,\r\n",
        "        # and Tensor objects are evaluated. A session may own resources, such as\r\n",
        "        # tf.Variable, tf.QueueBase and tf.ReaderBase. It is important to\r\n",
        "        # release these resources when they are no longer required. Invoke\r\n",
        "        # tf.Session.close method on the session or use the session as a context\r\n",
        "        # manager. \r\n",
        "        # with tf.Session() as sess:\r\n",
        "        #   sess.run(...)\r\n",
        "        # or\r\n",
        "        # sess = tf.Session()\r\n",
        "        # sess.run(...)\r\n",
        "        # sess.close()     \r\n",
        "        with tf.Session() as sess:\r\n",
        "            sess.run(tf.global_variables_initializer())\r\n",
        "            saver = tf.train.Saver()\r\n",
        "            saver.restore(sess, self.gan_model)\r\n",
        "            # Reconstruct a Python object from a file persisted with joblib.dump\r\n",
        "            clf = joblib.load(self.xgb_model)\r\n",
        "            for sym, date, data in self.data:\r\n",
        "                # run takes in feed_dict=None, session=None. A feed_dict is a\r\n",
        "                # dictionary that maps Tensor objects to feed values. In this\r\n",
        "                # case, I believe we are doing run( fetches, feed_dict=None...)\r\n",
        "                # case where the fetches is gan.features and the feed_dict\r\n",
        "                # points to the gan.X dictionary which points to data. The\r\n",
        "                # fetches argument may be a single graph element, or an\r\n",
        "                # arbitrarily nested list, tuple, namedtuple, dict, or\r\n",
        "                # OrderedDict containing graph elements at its leaves.\r\n",
        "#                 print(\"data: \", data)\r\n",
        "#                 print(\"data length: \", len(data))\r\n",
        "#                 print(\"gan features: \", gan.features)\r\n",
        "                features = sess.run(gan.features, feed_dict={gan.X:[data]})\r\n",
        "                # Value returned by run() has the same shape as the fetches\r\n",
        "                # argument, where the leaves are replaced by the corresponding\r\n",
        "                # values returned by TensorFlow.  \r\n",
        "                \r\n",
        "                # xgb.DMatrix, construct one from either a dense matrix, a\r\n",
        "                # sparse matrix, or a local file. Supported input file formats\r\n",
        "                # are either a libsvm text file or a binary file that was\r\n",
        "                # created previously by xgb.DMatrix.save. Internal data\r\n",
        "                # structure that is used by XGBoost which is optimized for both\r\n",
        "                # memory efficiency and training speed.\r\n",
        "                features = xgb.DMatrix(features)\r\n",
        "                \r\n",
        "\r\n",
        "                # The clf predict is the xgb classifier that is used on the gan\r\n",
        "                # features (the flattened last layer of the convolutional neural\r\n",
        "                # network, that is the discriminator). As far as I can tell, we\r\n",
        "                # are using the GAN on the past 20 days to come up with some\r\n",
        "                # features. Then these features are plugged into the XGBoost\r\n",
        "                # Classifier. Then the XGBoost Classifier makes a prediction for\r\n",
        "                # the stock (going Up or Down).\r\n",
        "                print('{} {} {}'.format(str(date).split(' ')[0], sym, clf.predict(features)[0][1] > 0.5))\r\n",
        "\r\n",
        "\r\n",
        "# if __name__ == '__main__':\r\n",
        "p = Predict(num_historical_days=HISTORICAL_DAYS_AMOUNT, days=DAYS_AHEAD, pct_change=PCT_CHANGE_AMOUNT)\r\n",
        "p.gan_predict()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BTC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/LTC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/NMC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/PPC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/ETH.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BNB.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/DOT.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/ADA.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/USDT.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XRP.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/LINK.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BCH.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XLM.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/UNI.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/USDC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/DOGE.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/WBTC.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/AAVE.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/ATOM.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/EOS.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XEM.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XMR.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/BSV.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/TRX.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/HT .csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/MIOTA.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/XTZ.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/VET.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/THETA.csv\n",
            "drive/My Drive/Colab Notebooks/GAN cryptocurrency/stock_data/CRO.csv\n",
            "Tensor(\"Relu:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_2:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"Relu_4:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_5:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_6:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/Colab Notebooks/GAN cryptocurrency/deployed_model/gan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "DataLossError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1360\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataLossError\u001b[0m: 2 root error(s) found.\n  (0) Data loss: Unable to open table file drive/My Drive/Colab Notebooks/GAN cryptocurrency/deployed_model/gan: Failed precondition: drive/My Drive/Colab Notebooks/GAN cryptocurrency/deployed_model/gan; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[{{node save/RestoreV2}}]]\n  (1) Data loss: Unable to open table file drive/My Drive/Colab Notebooks/GAN cryptocurrency/deployed_model/gan: Failed precondition: drive/My Drive/Colab Notebooks/GAN cryptocurrency/deployed_model/gan; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[{{node save/RestoreV2}}]]\n\t [[save/RestoreV2/_19]]\n0 successful operations.\n0 derived errors ignored.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-ce8168467594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# if __name__ == '__main__':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_historical_days\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHISTORICAL_DAYS_AMOUNT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDAYS_AHEAD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpct_change\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPCT_CHANGE_AMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgan_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-ce8168467594>\u001b[0m in \u001b[0;36mgan_predict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgan_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;31m# Reconstruct a Python object from a file persisted with joblib.dump\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1296\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1298\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1299\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m       \u001b[0;31m# There are three common conditions that might cause this error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1191\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1369\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1392\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1394\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataLossError\u001b[0m: 2 root error(s) found.\n  (0) Data loss: Unable to open table file drive/My Drive/Colab Notebooks/GAN cryptocurrency/deployed_model/gan: Failed precondition: drive/My Drive/Colab Notebooks/GAN cryptocurrency/deployed_model/gan; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[node save/RestoreV2 (defined at <ipython-input-33-ce8168467594>:73) ]]\n  (1) Data loss: Unable to open table file drive/My Drive/Colab Notebooks/GAN cryptocurrency/deployed_model/gan: Failed precondition: drive/My Drive/Colab Notebooks/GAN cryptocurrency/deployed_model/gan; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[node save/RestoreV2 (defined at <ipython-input-33-ce8168467594>:73) ]]\n\t [[save/RestoreV2/_19]]\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'save/RestoreV2':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 451, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 434, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-33-ce8168467594>\", line 115, in <module>\n    p.gan_predict()\n  File \"<ipython-input-33-ce8168467594>\", line 73, in gan_predict\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 835, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 847, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 885, in _build\n    build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 515, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 335, in _AddRestoreOps\n    restore_sequentially)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 582, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1511, in restore_v2\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 750, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3536, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1990, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
          ]
        }
      ]
    }
  ]
}